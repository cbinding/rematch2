{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import warnings\n",
    "# suppress user warnings during execution\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "# load required dependencies\n",
    "%pip install --upgrade pip\n",
    "%pip install pandas urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached file ./data/fish//ArchaeologicalObjectsV28.zip\n",
      "Using cached file ./data/fish//ArchaeologicalSciencesV28.zip\n",
      "Using cached file ./data/fish//BuildingMaterialsV28.zip\n",
      "Using cached file ./data/fish//ComponentsV28.zip\n",
      "Using cached file ./data/fish//EventV28.zip\n",
      "Using cached file ./data/fish//EvidenceV28.zip\n",
      "Using cached file ./data/fish//MaritimeCraftV28.zip\n",
      "Using cached file ./data/fish//MonumentV28.zip\n"
     ]
    }
   ],
   "source": [
    "# script to download latest FISH vocabularies from heritage-standards.org.uk/fish-vocabularies/\n",
    "# process them and output JSON files of spaCy NER patterns for use in entity recognition\n",
    "\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "from slugify import slugify\n",
    "import pandas\n",
    "from urllib.parse import urlparse, unquote, quote_plus\n",
    "\n",
    "# HeritageData used a standardised list of scheme_id for vocabularies, \n",
    "# _sometimes_ different to the (numeric) CLA_GR_UID as used in AMIE DB\n",
    "# Convert a vocabulary ID to the corresponding HeritageData scheme_id\n",
    "def vocabulary_id_to_heritagedata_scheme_id(vocabulary_id: str) -> str:\n",
    "    mapping = {\n",
    "        \"1\": \"eh_tmt2\",\n",
    "        \"92\": \"eh_evd\",\n",
    "        \"128\": \"mda_obj\",\n",
    "        \"129\": \"eh_tbm\",\n",
    "        \"143\": \"eh_tmc\",\n",
    "        \"546\": \"eh_com\",\n",
    "        \"566\": \"agl_et\",        \n",
    "    }\n",
    "    return mapping.get(vocabulary_id, vocabulary_id)\n",
    "\n",
    "\n",
    "# download file from URL to output path and return filename including path\n",
    "# use a previously cached file if it exists, rather than downloading again\n",
    "def download_file_from_url(url: str, output_path: str=\".\") -> str:    \n",
    "\n",
    "    # ensure the intended output path exists   \n",
    "    if not os.path.exists(output_path): os.makedirs(output_path)\n",
    "\n",
    "    # just use previously cached file if it exists\n",
    "    file_name = url.split(\"?\")[0].split(\"/\")[-1]\n",
    "    file_path = f\"{output_path}/{file_name}\" \n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Using cached file {file_path}\")        \n",
    "    else:\n",
    "        print(f\"Downloading file {url} to {file_path}\")\n",
    "        response = requests.get(url, timeout=30, stream=True)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "# get latest zip files of CSV data from https://heritage-standards.org.uk/fish-vocabularies/ \n",
    "# Note the URLs for the 'latest' files do change so check the URL above for the latest links \n",
    "remote_file_urls = [\n",
    "    r\"https://heritage-standards.org.uk/2025/zip_files/ArchaeologicalObjectsV28.zip\",\n",
    "    r\"https://heritage-standards.org.uk/2025/zip_files/ArchaeologicalSciencesV28.zip\",\n",
    "    r\"https://heritage-standards.org.uk/2025/zip_files/BuildingMaterialsV28.zip\",\n",
    "    r\"https://heritage-standards.org.uk/2025/zip_files/ComponentsV28.zip\",\n",
    "    r\"https://heritage-standards.org.uk/2025/zip_files/EventV28.zip\",\n",
    "    r\"https://heritage-standards.org.uk/2025/zip_files/EvidenceV28.zip\",\n",
    "    r\"https://heritage-standards.org.uk/2025/zip_files/MaritimeCraftV28.zip\",\n",
    "    r\"https://heritage-standards.org.uk/2025/zip_files/MonumentV28.zip\"    \n",
    "]\n",
    "\n",
    "# ensure local data directory exists first\n",
    "local_directory = r\"./data/fish/\"\n",
    "\n",
    "for url in remote_file_urls:    \n",
    "    # download and cache the zip file locally\n",
    "    local_file_path = download_file_from_url(url, local_directory)\n",
    "\n",
    "    # create directory to extract zip contents into\n",
    "    local_file_name = url.split(\"?\")[0].split(\"/\")[-1]\n",
    "    local_data_path = os.path.join(local_directory, slugify(local_file_name))\n",
    "    if not os.path.exists(local_data_path):\n",
    "        os.makedirs(local_data_path)\n",
    "\n",
    "    # extract zip contents locally to named directory   \n",
    "    with zipfile.ZipFile(local_file_path, 'r') as zf:\n",
    "        zf.extractall(local_data_path)\n",
    "    \n",
    "    # read only the data files we are interested in for this exercise\n",
    "    terms_file_name = os.path.join(local_data_path, \"thesaurus_terms.csv\")\n",
    "    df = pandas.read_csv(terms_file_name, encoding_errors=\"replace\")\n",
    "    terms = df.to_dict(orient=\"records\")\n",
    "    \n",
    "    prefs_file_name = os.path.join(local_data_path, \"thesaurus_term_preferences.csv\")\n",
    "    df = pandas.read_csv(prefs_file_name, encoding_errors=\"replace\")\n",
    "    prefs = df.to_dict(orient=\"records\")\n",
    "    \n",
    "    # generate a list of spaCy patterns for use in the NER \n",
    "    data = []\n",
    "    scheme_id = \"unknown\"\n",
    "\n",
    "    for term in terms:\n",
    "        vocabulary_id = str(term.get(\"CLA_GR_UID\", \"unknown\")).strip()\n",
    "        scheme_id = quote_plus(vocabulary_id_to_heritagedata_scheme_id(vocabulary_id)) \n",
    "        term_id = quote_plus(str(term.get(\"THE_TE_UID\", \"unknown\")).strip())\n",
    "        preferred = next((item for item in prefs if str(item.get(\"THE_TE_UID_1\", \"\")) == term_id), None)\n",
    "        if(preferred is not None): term_id = quote_plus(str(preferred.get(\"THE_TE_UID_2\", term_id)).strip())\n",
    "\n",
    "        term_label = str(term.get(\"TERM\", \"\")).strip().title() # title case for diff...\n",
    "        concept_id = f\"http://purl.org/heritagedata/schemes/{scheme_id}/concepts/{term_id}\"\n",
    "        data.append({\"id\": concept_id, \"pattern\": term_label })\n",
    "    \n",
    "    # write out the patterns to a JSON file\n",
    "    output_file_name = os.path.join(local_directory, f\"patterns_en_FISH_{scheme_id}.json\")  \n",
    "    with open(output_file_name, \"w\") as fp: #, encoding=\"utf-8\"\n",
    "        json.dump(data, fp, indent=2)\n",
    "\n",
    "    # clean up intermediate files (but keeping cached zip files)\n",
    "    shutil.rmtree(local_data_path, ignore_errors=True)  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
