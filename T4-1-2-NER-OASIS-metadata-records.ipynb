{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process OASIS records\n",
    "Vocabulary-based Named Entity Recognition (NER) applied to a set of XML OASIS abstracts obtained from ADS. Detecting temporal phrases and object/monument types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import warnings\n",
    "# suppress user warnings during execution\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "# load required dependencies\n",
    "%pip install --upgrade pip\n",
    "%pip install spacy\n",
    "%pip install ipywidgets\n",
    "%sx python -m spacy download en_core_web_sm\n",
    "\n",
    "#from IPython.display import display, HTML\n",
    "from slugify import slugify # for valid filenames from identifiers\n",
    "import spacy # for NER processing\n",
    "from spacy.tokens import Doc # for NER results\n",
    "from lxml import etree as ET # for parsing input records from XML file\n",
    "from datetime import datetime as DT # for timestamps\n",
    "from html import escape # for writing escaped HTML\n",
    "import pandas as pd  # for DataFrame\n",
    "import os\n",
    "from rematch2 import PeriodoRuler, VocabularyRuler, NegationRuler, DocSummary, StringCleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# parse and extract a list of OASIS abstract records from source XML file \n",
    "# returns [{\"id\", \"text\"}, {\"id\", \"text\"}, ...] for subsequent processing\n",
    "def get_records_from_xml_file(file_path: str=\"\") -> list:\n",
    "    records = []\n",
    "    try:\n",
    "        # read XML file\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "    except:\n",
    "        print(f\"Could not read from {file_path}\")\n",
    "        return []\n",
    "\n",
    "    # find rows to be processed in the XML file\n",
    "    rows = tree.xpath(\"/table/rows/row\")\n",
    "\n",
    "    for row in rows:\n",
    "        # find abstract(s) in the current item\n",
    "        abstracts = row.xpath(\"value[@columnNumber='1']/text()\")\n",
    "       \n",
    "        # if multiple abstracts, get first one\n",
    "        if (len(abstracts) > 0):\n",
    "            abstract = abstracts[0]\n",
    "        else:\n",
    "            abstract = \"\"\n",
    "\n",
    "         # find identifier(s) in the current item\n",
    "        identifiers = row.xpath(\"value[@columnNumber='0']/text()\")\n",
    "\n",
    "        # if multiple identifiers, get first one (remove URL prefix if present)\n",
    "        if (len(identifiers) > 0):\n",
    "            identifier = identifiers[0]\n",
    "            identifier = identifier.replace(\n",
    "                \"https://archaeologydataservice.ac.uk/archsearch/record?titleId=\", \"\")\n",
    "        else:\n",
    "            identifier = \"\"\n",
    "\n",
    "        ## create new (cleaned) record and add it\n",
    "        record = {}\n",
    "        record[\"id\"] = str(identifier).strip()\n",
    "        record[\"text\"] = str(abstract).strip()\n",
    "        records.append(record)\n",
    "\n",
    "    # finally, return the extracted list\n",
    "    return records\n",
    "\n",
    "\n",
    "# parse and extract a list of OASIS abstract records from source CSV file \n",
    "# returns [{\"id\", \"text\"}, {\"id\", \"text\"}, ...] for subsequent processing\n",
    "def get_records_from_csv_file(file_path: str=\"\") -> list:\n",
    "    records = []\n",
    "    \n",
    "    # read the CSV file to a DataFrame\n",
    "    df = pd.read_csv(file_path, skip_blank_lines=True)\n",
    "    # set any NaN values to blank string\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    # convert the data to a dict structure\n",
    "    items = df.to_dict(orient=\"records\") \n",
    "    \n",
    "    records = list(map(lambda item: { \n",
    "        \"id\": str(item[\"file\"]).strip(), \n",
    "        \"text\": str(item[\"abstract\"]).strip() }, items))\n",
    "    \n",
    "    return records\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# use predefined spaCy pipeline (English), no NER\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable = ['ner'])\n",
    "\n",
    "# using HE Periods list\n",
    "periodo_authority_id = \"p0kh9ds\" \n",
    "\n",
    "# add rematch2 NER components to the end of the pipeline\n",
    "nlp.add_pipe(\"yearspan_ruler\", last=True)    \n",
    "nlp.add_pipe(\"periodo_ruler\", last=True, config={\"periodo_authority_id\": periodo_authority_id}) \n",
    "nlp.add_pipe(\"fish_archobjects_ruler\", last=True)\n",
    "nlp.add_pipe(\"fish_monument_types_ruler\", last=True)  \n",
    "nlp.add_pipe(\"fish_supplementary_ruler\", last=True) \n",
    "nlp.add_pipe(\"negation_ruler\", last=True) \n",
    "\n",
    "# process ADS CSV report examples\n",
    "input_file_path = \"./data/report_metadata\"\n",
    "input_file_name = \"report_metadata.csv\"\n",
    "\n",
    "# process ADS CSV journal examples\n",
    "#input_file_path = \"./data/journal_metadata\"\n",
    "#input_file_name = \"journal_metadata.csv\"\n",
    "\n",
    "# process ADS XML metadata examples\n",
    "# input_file_path = \"./data/oasis_descr_examples\"\n",
    "# input_file_name = \"oasis_descr_examples.xml\"\n",
    "\n",
    "input_records = []\n",
    "input_file = os.path.join(input_file_path, input_file_name)\n",
    "if input_file.lower().endswith(\"xml\"):\n",
    "    input_records = get_records_from_xml_file(input_file)\n",
    "elif input_file.lower().endswith(\"csv\"):\n",
    "    input_records = get_records_from_csv_file(input_file)\n",
    "\n",
    "record_count = len(input_records)\n",
    "\n",
    "metadata = {\n",
    "    \"identifier\": \"\",\n",
    "    \"title\": \"vocabulary-based NER results\",\n",
    "    \"description\": \"vocabulary-based NER annotation on text abstracts\",\n",
    "    \"creator\": \"T4-1-2-NER-OASIS-metadata-records.ipynb\",\n",
    "    \"created\": DT.now().strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "    \"periodo_authority_id\": periodo_authority_id,\n",
    "    \"ner_pipeline\": nlp.pipe_names,\n",
    "    \"input_file_name\": input_file_name,\n",
    "    \"input_record_count\": record_count        \n",
    "}\n",
    "\n",
    "# create output file path if it does not already exist\n",
    "output_file_path = os.path.join(input_file_path, \"output\")\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.makedirs(output_file_path)\n",
    "\n",
    "current_record = 0    \n",
    "for record in input_records or []:        \n",
    "    current_record += 1\n",
    "\n",
    "    # get ID and text from the record\n",
    "    identifier = record.get(\"id\", \"\").strip()\n",
    "    input_text = record.get(\"text\", \"\")\n",
    "\n",
    "    metadata[\"identifier\"] = identifier\n",
    "\n",
    "    # print progress indicator\n",
    "    print(f\"processing record {current_record} of {record_count} [ID: {identifier}]\")\n",
    "        \n",
    "    # normalise text prior to NER processing\n",
    "    # (whitespace, punctuation & spelling)\n",
    "    cleaned = StringCleaning.normalize(input_text)\n",
    "\n",
    "    # perform annotation on cleaned text    \n",
    "    doc = nlp(cleaned)\n",
    "    summary = DocSummary(doc, metadata=metadata)\n",
    "\n",
    "    # build output file names incorporating record identifier\n",
    "    # slugify identifier in case of bad characters for file names\n",
    "    html_file = os.path.join(output_file_path, f\"{slugify(identifier)}.html\")\n",
    "    text_file = os.path.join(output_file_path, f\"{slugify(identifier)}.txt\")\n",
    "    json_file = os.path.join(output_file_path, f\"{slugify(identifier)}.json\")\n",
    "\n",
    "    # write results to HTML, TEXT and JSON files    \n",
    "    with open(html_file, \"w\") as file:\n",
    "        file.write(summary.report(format=\"html\"))\n",
    "    with open(text_file, \"w\") as file:\n",
    "        file.write(summary.report(format=\"text\"))    \n",
    "    with open(json_file, \"w\") as file:\n",
    "        file.write(summary.report(format=\"json\")) \n",
    "    \n",
    "    # temp interrupt after a few records (while testing)\n",
    "    #if current_record == 5:\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# build list of results\n",
    "def result_link(record):\n",
    "    identifier = record[\"id\"] \n",
    "    file_path=f\"https://html-preview.github.io/?url=https://github.com/cbinding/rematch2/blob/main/data/output/{slugify(identifier)}.html\"\n",
    "    return f\"<li><a href='{file_path}'>{identifier}</a></li>\" \n",
    "results = list(map(result_link, input_records or []))\n",
    "results.sort()\n",
    "#display(HTML(\"<ul>\" + \"\".join(results) + \"</ul>\"))\n",
    "with open(\"./data/output/results.md\", \"w\") as file:\n",
    "    file.write(\"<ul>\" + \"\".join(results) + \"</ul>\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
