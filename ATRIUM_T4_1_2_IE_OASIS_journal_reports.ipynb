{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running ATRIUM information extraction pipeline on full text extracted from OASIS PDF reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "# suppress user warnings during execution\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "# load required dependencies\n",
    "%pip install --upgrade pip\n",
    "%pip install spacy\n",
    "%pip install ipywidgets\n",
    "\n",
    "%sx python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up nlp pipeline\n",
      "finished setting up nlp pipeline in 0:01:45.686281\n",
      "extracting metadata records from CSV file\n",
      "Total metadata records extracted: 75\n",
      "scanning input file directory\n",
      "--------------------------------\n",
      "reading text_extraction_surreyac103_091-172_haslam.pdf.json\n",
      "processing file 1: text_extraction_surreyac103_091-172_haslam.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:04:38.276556\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000121\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.197467\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 367.597 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 41.805 seconds\n",
      "\"_get_all_pairs\" ran in 409.403 seconds\n",
      "\"get_span_pairs\" ran in 409.403 seconds\n",
      "\"report_to_html\" ran in 427.226 seconds\n",
      "\"report\" ran in 427.226 seconds\n",
      "finished creating PDF report in 0:07:30.628543\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.446 seconds\n",
      "\"report\" ran in 0.446 seconds\n",
      "finished creating JSON report in 0:00:00.774209\n",
      "--------------------------------\n",
      "reading text_extraction_SAC118_Stevens.pdf.json\n",
      "processing file 2: text_extraction_SAC118_Stevens.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:41.188784\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.004335\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.017454\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 7.425 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 0.866 seconds\n",
      "\"_get_all_pairs\" ran in 8.292 seconds\n",
      "\"get_span_pairs\" ran in 8.292 seconds\n",
      "\"report_to_html\" ran in 8.657 seconds\n",
      "\"report\" ran in 8.657 seconds\n",
      "finished creating PDF report in 0:00:13.360616\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.065 seconds\n",
      "\"report\" ran in 0.065 seconds\n",
      "finished creating JSON report in 0:00:00.112035\n",
      "--------------------------------\n",
      "reading text_extraction_archael547-079-116-ceolwulf.pdf.json\n",
      "processing file 3: text_extraction_archael547-079-116-ceolwulf.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:02:08.281605\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000563\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.025621\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 37.621 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 4.693 seconds\n",
      "\"_get_all_pairs\" ran in 42.315 seconds\n",
      "\"get_span_pairs\" ran in 42.315 seconds\n",
      "\"report_to_html\" ran in 44.354 seconds\n",
      "\"report\" ran in 44.354 seconds\n",
      "finished creating PDF report in 0:00:50.693735\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.615 seconds\n",
      "\"report\" ran in 0.615 seconds\n",
      "finished creating JSON report in 0:00:00.753559\n",
      "--------------------------------\n",
      "reading text_extraction_2022_96_079-094_Browning_et_al.pdf.json\n",
      "processing file 4: text_extraction_2022_96_079-094_Browning_et_al.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:34.066308\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.003123\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.019505\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 6.052 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 0.802 seconds\n",
      "\"_get_all_pairs\" ran in 6.855 seconds\n",
      "\"get_span_pairs\" ran in 6.855 seconds\n",
      "\"report_to_html\" ran in 7.190 seconds\n",
      "\"report\" ran in 7.190 seconds\n",
      "finished creating PDF report in 0:00:10.618930\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.085 seconds\n",
      "\"report\" ran in 0.085 seconds\n",
      "finished creating JSON report in 0:00:00.124539\n",
      "--------------------------------\n",
      "reading text_extraction_DAJ_v106_1986_005-017.pdf.json\n",
      "processing file 5: text_extraction_DAJ_v106_1986_005-017.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:28.487500\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000380\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.013197\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 3.929 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 0.484 seconds\n",
      "\"_get_all_pairs\" ran in 4.414 seconds\n",
      "\"get_span_pairs\" ran in 4.414 seconds\n",
      "\"report_to_html\" ran in 4.669 seconds\n",
      "\"report\" ran in 4.669 seconds\n",
      "finished creating PDF report in 0:00:07.953402\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.332 seconds\n",
      "\"report\" ran in 0.332 seconds\n",
      "finished creating JSON report in 0:00:00.364154\n",
      "--------------------------------\n",
      "reading text_extraction_120_215_235.pdf.json\n",
      "processing file 6: text_extraction_120_215_235.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:43.895223\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000375\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.022948\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 12.123 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 1.546 seconds\n",
      "\"_get_all_pairs\" ran in 13.669 seconds\n",
      "\"get_span_pairs\" ran in 13.669 seconds\n",
      "\"report_to_html\" ran in 14.181 seconds\n",
      "\"report\" ran in 14.181 seconds\n",
      "finished creating PDF report in 0:00:18.877805\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.073 seconds\n",
      "\"report\" ran in 0.073 seconds\n",
      "finished creating JSON report in 0:00:00.144183\n",
      "--------------------------------\n",
      "reading text_extraction_archael547-005-040-breeze.pdf.json\n",
      "processing file 7: text_extraction_archael547-005-040-breeze.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:02:15.564897\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000572\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.055518\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 69.638 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 8.111 seconds\n",
      "\"_get_all_pairs\" ran in 77.749 seconds\n",
      "\"get_span_pairs\" ran in 77.749 seconds\n",
      "\"report_to_html\" ran in 80.870 seconds\n",
      "\"report\" ran in 80.870 seconds\n",
      "finished creating PDF report in 0:01:29.681586\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.763 seconds\n",
      "\"report\" ran in 0.763 seconds\n",
      "finished creating JSON report in 0:00:00.915576\n",
      "--------------------------------\n",
      "reading text_extraction_NAS_20_1985_113-138_Shaw.pdf.json\n",
      "processing file 8: text_extraction_NAS_20_1985_113-138_Shaw.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:01:31.828250\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.002920\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.038727\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 35.195 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 3.629 seconds\n",
      "\"_get_all_pairs\" ran in 38.824 seconds\n",
      "\"get_span_pairs\" ran in 38.825 seconds\n",
      "\"report_to_html\" ran in 40.570 seconds\n",
      "\"report\" ran in 40.570 seconds\n",
      "finished creating PDF report in 0:00:48.213513\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.140 seconds\n",
      "\"report\" ran in 0.140 seconds\n",
      "finished creating JSON report in 0:00:00.242256\n",
      "--------------------------------\n",
      "reading text_extraction_surreyac103_185-266_saxby.pdf.json\n",
      "processing file 9: text_extraction_surreyac103_185-266_saxby.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:02:41.346917\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000759\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.087096\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 179.505 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 19.076 seconds\n",
      "\"_get_all_pairs\" ran in 198.582 seconds\n",
      "\"get_span_pairs\" ran in 198.582 seconds\n",
      "\"report_to_html\" ran in 207.384 seconds\n",
      "\"report\" ran in 207.384 seconds\n",
      "finished creating PDF report in 0:03:46.834777\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.296 seconds\n",
      "\"report\" ran in 0.296 seconds\n",
      "finished creating JSON report in 0:00:00.536088\n",
      "--------------------------------\n",
      "reading text_extraction_DAJ_v106_1986_018-100.pdf.json\n",
      "processing file 10: text_extraction_DAJ_v106_1986_018-100.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:03:57.040454\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.001757\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.072109\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 205.311 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 22.945 seconds\n",
      "\"_get_all_pairs\" ran in 228.256 seconds\n",
      "\"get_span_pairs\" ran in 228.256 seconds\n",
      "\"report_to_html\" ran in 238.402 seconds\n",
      "\"report\" ran in 238.402 seconds\n",
      "finished creating PDF report in 0:04:13.538043\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.369 seconds\n",
      "\"report\" ran in 0.369 seconds\n",
      "finished creating JSON report in 0:00:00.650518\n",
      "--------------------------------\n",
      "reading text_extraction_DAJ_v023_1901_040-047.pdf.json\n",
      "processing file 11: text_extraction_DAJ_v023_1901_040-047.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:13.159971\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.009566\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.005724\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 0.544 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 0.057 seconds\n",
      "\"_get_all_pairs\" ran in 0.601 seconds\n",
      "\"get_span_pairs\" ran in 0.601 seconds\n",
      "\"report_to_html\" ran in 0.636 seconds\n",
      "\"report\" ran in 0.636 seconds\n",
      "finished creating PDF report in 0:00:02.988878\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.020 seconds\n",
      "\"report\" ran in 0.020 seconds\n",
      "finished creating JSON report in 0:00:00.034229\n",
      "--------------------------------\n",
      "reading text_extraction_078_233_250.pdf.json\n",
      "processing file 12: text_extraction_078_233_250.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:44.630550\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000180\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.011065\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 5.034 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 0.712 seconds\n",
      "\"_get_all_pairs\" ran in 5.746 seconds\n",
      "\"get_span_pairs\" ran in 5.746 seconds\n",
      "\"report_to_html\" ran in 6.003 seconds\n",
      "\"report\" ran in 6.003 seconds\n",
      "finished creating PDF report in 0:00:08.812439\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.060 seconds\n",
      "\"report\" ran in 0.060 seconds\n",
      "finished creating JSON report in 0:00:00.111305\n",
      "--------------------------------\n",
      "reading text_extraction_2022_96_013-068_Huxley.pdf.json\n",
      "processing file 13: text_extraction_2022_96_013-068_Huxley.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:01:34.187159\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000381\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.062604\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 75.833 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 8.641 seconds\n",
      "\"_get_all_pairs\" ran in 84.476 seconds\n",
      "\"get_span_pairs\" ran in 84.477 seconds\n",
      "\"report_to_html\" ran in 87.968 seconds\n",
      "\"report\" ran in 87.968 seconds\n",
      "finished creating PDF report in 0:01:41.540296\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.241 seconds\n",
      "\"report\" ran in 0.242 seconds\n",
      "finished creating JSON report in 0:00:00.372618\n",
      "--------------------------------\n",
      "reading text_extraction_archael522-067-077-whitworth.pdf.json\n",
      "processing file 14: text_extraction_archael522-067-077-whitworth.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:26.408720\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.004321\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.011612\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 2.905 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 0.404 seconds\n",
      "\"_get_all_pairs\" ran in 3.309 seconds\n",
      "\"get_span_pairs\" ran in 3.309 seconds\n",
      "\"report_to_html\" ran in 3.476 seconds\n",
      "\"report\" ran in 3.476 seconds\n",
      "finished creating PDF report in 0:00:06.851849\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.315 seconds\n",
      "\"report\" ran in 0.315 seconds\n",
      "finished creating JSON report in 0:00:00.347752\n",
      "--------------------------------\n",
      "reading text_extraction_DAJ_v086_1966_093-098.pdf.json\n",
      "processing file 15: text_extraction_DAJ_v086_1966_093-098.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:10.251857\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000361\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.006745\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 0.549 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 0.059 seconds\n",
      "\"_get_all_pairs\" ran in 0.608 seconds\n",
      "\"get_span_pairs\" ran in 0.608 seconds\n",
      "\"report_to_html\" ran in 0.646 seconds\n",
      "\"report\" ran in 0.646 seconds\n",
      "finished creating PDF report in 0:00:02.186971\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.021 seconds\n",
      "\"report\" ran in 0.021 seconds\n",
      "finished creating JSON report in 0:00:00.036212\n",
      "--------------------------------\n",
      "reading text_extraction_2022_96_001_012_Cooper_Garton.pdf.json\n",
      "processing file 16: text_extraction_2022_96_001_012_Cooper_Garton.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:24.469608\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000175\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.012705\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 3.290 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 0.347 seconds\n",
      "\"_get_all_pairs\" ran in 3.638 seconds\n",
      "\"get_span_pairs\" ran in 3.638 seconds\n",
      "\"report_to_html\" ran in 3.835 seconds\n",
      "\"report\" ran in 3.835 seconds\n",
      "finished creating PDF report in 0:00:06.898030\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.038 seconds\n",
      "\"report\" ran in 0.038 seconds\n",
      "finished creating JSON report in 0:00:00.071513\n",
      "--------------------------------\n",
      "reading text_extraction_SAC118_Bedwin.pdf.json\n",
      "processing file 17: text_extraction_SAC118_Bedwin.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:22.049409\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000328\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.012493\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 2.904 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 0.350 seconds\n",
      "\"_get_all_pairs\" ran in 3.254 seconds\n",
      "\"get_span_pairs\" ran in 3.255 seconds\n",
      "\"report_to_html\" ran in 3.401 seconds\n",
      "\"report\" ran in 3.401 seconds\n",
      "finished creating PDF report in 0:00:06.406667\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.036 seconds\n",
      "\"report\" ran in 0.036 seconds\n",
      "finished creating JSON report in 0:00:00.064419\n",
      "--------------------------------\n",
      "reading text_extraction_surreyac103_297-305_english.pdf.json\n",
      "processing file 18: text_extraction_surreyac103_297-305_english.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:27.843861\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000381\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.010957\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 3.148 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 0.333 seconds\n",
      "\"_get_all_pairs\" ran in 3.481 seconds\n",
      "\"get_span_pairs\" ran in 3.481 seconds\n",
      "\"report_to_html\" ran in 3.667 seconds\n",
      "\"report\" ran in 3.668 seconds\n",
      "finished creating PDF report in 0:00:06.621269\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.039 seconds\n",
      "\"report\" ran in 0.039 seconds\n",
      "finished creating JSON report in 0:00:00.074522\n",
      "--------------------------------\n",
      "reading text_extraction_120_031_097.pdf.json\n",
      "processing file 19: text_extraction_120_031_097.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:03:20.232493\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000413\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.031906\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 76.042 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 8.285 seconds\n",
      "\"_get_all_pairs\" ran in 84.327 seconds\n",
      "\"get_span_pairs\" ran in 84.327 seconds\n",
      "\"report_to_html\" ran in 87.657 seconds\n",
      "\"report\" ran in 87.657 seconds\n",
      "finished creating PDF report in 0:01:36.745757\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.253 seconds\n",
      "\"report\" ran in 0.253 seconds\n",
      "finished creating JSON report in 0:00:00.463084\n",
      "--------------------------------\n",
      "reading text_extraction_SAC118_Garton.pdf.json\n",
      "processing file 20: text_extraction_SAC118_Garton.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:21.010929\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.008616\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.009473\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 1.994 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 0.190 seconds\n",
      "\"_get_all_pairs\" ran in 2.185 seconds\n",
      "\"get_span_pairs\" ran in 2.185 seconds\n",
      "\"report_to_html\" ran in 2.282 seconds\n",
      "\"report\" ran in 2.282 seconds\n",
      "finished creating PDF report in 0:00:04.899525\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.033 seconds\n",
      "\"report\" ran in 0.033 seconds\n",
      "finished creating JSON report in 0:00:00.059329\n",
      "finished processing 20 files in 0:53:34.534846\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime as DT # for timestamps\n",
    "import json, os\n",
    "from slugify import slugify # for valid filenames from identifiers\n",
    "from weasyprint import HTML\n",
    "#from rematch2 import PeriodoRuler, VocabularyRuler, NegationRuler, DocSummary, TextNormalizer\n",
    "from rematch2 import DocSummary\n",
    "from rematch2.SpanScorer import SpanScorer\n",
    "#from tides_dataclasses import Section\n",
    "\n",
    "from ATRIUM_T4_1_2_IE_pipeline import get_pipeline, read_csv_file\n",
    "\n",
    "# using predefined spaCy pipeline (English)\n",
    "ts_started = DT.now()     \n",
    "print(\"setting up nlp pipeline\")\n",
    "nlp = get_pipeline(\"en\")\n",
    "print(f\"finished setting up nlp pipeline in {DT.now() - ts_started}\")\n",
    "\n",
    "# input directory containing text files to process\n",
    "#input_directory = \"./data/oasis/journals_july_2024/text extraction - new\" # Mugdha's output\n",
    "input_directory = \"./data/oasis/journals_july_2024/text_extraction_20251117\" # Mark's script re-extracted text 2025-11-17\n",
    "\n",
    "# read separate CSV file containing titles and abstracts for (some of) the files we will process\n",
    "print(\"extracting metadata records from CSV file\")\n",
    "metadata_file_path = os.path.join(input_directory, \"journal_metadata.csv\")\n",
    "metadata_records = read_csv_file(file_path=metadata_file_path)\n",
    "print(f\"Total metadata records extracted: {len(metadata_records)}\")\n",
    "#print(metadata_records[0])  # print first record for inspection\n",
    "\n",
    "# subset of files to process\n",
    "file_names_subset = [\n",
    "\t\"text_extraction_120_031_097.pdf.json\",\n",
    "\t\"text_extraction_2022_96_013-068_Huxley.pdf.json\",\n",
    "    \"text_extraction_078_233_250.pdf.json\",\n",
    "    \"text_extraction_120_215_235.pdf.json\",\n",
    "    \"text_extraction_2022_96_001_012_Cooper_Garton.pdf.json\",\n",
    "    \"text_extraction_2022_96_079-094_Browning_et_al.pdf.json\",\n",
    "    \"text_extraction_archael522-067-077-whitworth.pdf.json\",\n",
    "    \"text_extraction_archael547-005-040-breeze.pdf.json\",\n",
    "    \"text_extraction_archael547-079-116-ceolwulf.pdf.json\",\n",
    "    \"text_extraction_DAJ_v023_1901_040-047.pdf.json\",\n",
    "    \"text_extraction_DAJ_v086_1966_093-098.pdf.json\",\n",
    "    \"text_extraction_DAJ_v106_1986_005-017.pdf.json\",\n",
    "    \"text_extraction_DAJ_v106_1986_018-100.pdf.json\",\n",
    "    \"text_extraction_NAS_20_1985_113-138_Shaw.pdf.json\",\n",
    "    \"text_extraction_SAC118_Bedwin.pdf.json\",\n",
    "    \"text_extraction_SAC118_Garton.pdf.json\",\n",
    "    \"text_extraction_SAC118_Stevens.pdf.json\",\n",
    "    \"text_extraction_surreyac103_091-172_haslam.pdf.json\",\n",
    "    \"text_extraction_surreyac103_185-266_saxby.pdf.json\",\n",
    "    \"text_extraction_surreyac103_297-305_english.pdf.json\",\n",
    "]\n",
    "\n",
    "# timestamp for use in directory names\n",
    "timestamp = ts_started.strftime('%Y%m%d')   \n",
    "\n",
    "# create output file path if it does not already exist\n",
    "output_directory = os.path.join(input_directory, f\"ie-output-{timestamp}\")\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "counter = 0\n",
    "print(\"scanning input file directory\")\n",
    "for entry in os.scandir(input_directory):\n",
    "    \n",
    "    if not entry.is_file(): continue\n",
    "    if not entry.name.lower() in list(map(str.lower, file_names_subset)): continue\n",
    "    # temp break for testing\n",
    "    #if counter >= 2: \n",
    "        #print(f\"reached test limit of {counter} files - stopping\")\n",
    "        #break\n",
    "    counter += 1     \n",
    "       \n",
    "    # read contents of input (JSON) file\n",
    "    print(f\"--------------------------------\")\n",
    "    print(f\"reading {entry.name}\")\n",
    "    input_file_content = {}        \n",
    "    with open(entry.path) as input_file:\n",
    "        if(entry.name.endswith(\".json\")):\n",
    "            input_file_content = json.load(input_file)\n",
    "        else:\n",
    "            input_file_content = { \"text\": input_file.read() }\n",
    "         \n",
    "    # find matching metadata record (if it exists) for this file. The metadata filename is the original PDF file; \n",
    "    # the input file is prefixed \"text_extraction_\" and suffixed \".json\" so find an entry that _contains_ the metadata filename    \n",
    "    metadata_record = next((record for record in metadata_records if record.get(\"file\", \"\").lower() in entry.name.lower()), {})\n",
    "        \n",
    "    # remove any existing title or abstract sections from input file content            \n",
    "    sections = input_file_content.get(\"sections\", [])\n",
    "    sections = list(filter(lambda sec: sec.get(\"type\", \"\") not in [\"title\", \"abstract\"], sections))\n",
    "\n",
    "    # get title and abstract from metadata record (if present)\n",
    "    title_text = metadata_record.get(\"title\", \"\").strip()\n",
    "    abstract_text = metadata_record.get(\"abstract\", \"\").strip()\n",
    "    \n",
    "    # create new title section (even if empty)\n",
    "    title_start = 0\n",
    "    title_end = title_start + (len(title_text) - 1 if title_text != \"\" else 0)\n",
    "    sec_title = {\n",
    "        \"type\": \"title\",\n",
    "        \"start\": title_start,\n",
    "        \"end\": title_end\n",
    "    }\n",
    "\n",
    "    # create new abstract section (even if empty)\n",
    "    abstract_start = sec_title[\"end\"] + (2 if title_text != \"\" else 0)  # add 2 to account for newline after title\n",
    "    abstract_end = abstract_start + (len(abstract_text) - 1 if abstract_text != \"\" else 0)\n",
    "    sec_abstract = {\n",
    "        \"type\": \"abstract\",\n",
    "        \"start\": abstract_start,\n",
    "        \"end\": abstract_end\n",
    "    }\n",
    "    # else: \n",
    "        # no abstract - so just use first page text to represent the abstract\n",
    "        #pages = list(filter(lambda sec: sec.get(\"type\", \"\") == \"page\", input_file_content.get(\"sections\", [])))\n",
    "        #if pages:\n",
    "            #first_page = min(pages, key=lambda p: p['start'])\n",
    "            #first_page_start = first_page.get(\"start\", 0)\n",
    "            #first_page_end = first_page.get(\"end\", 0)\n",
    "            #first_page_text = input_file_content.get(\"text\", \"\")[first_page_start:first_page_end]\n",
    "            #input_file_content[\"abstract\"] = first_page_text\n",
    "            #sec_abstract = {\n",
    "                #\"type\": \"abstract\",\n",
    "                #\"text\": first_page_text,\n",
    "                #\"start\": first_page_start,\n",
    "                #\"end\": first_page_end\n",
    "            #} \n",
    "            \n",
    "    # prepend title and abstract from metadata file (if available) to input file text\n",
    "    prepend_text = f\"{title_text}\\n{abstract_text}\"\n",
    "    prepend_size = len(prepend_text)\n",
    "    input_file_content[\"text\"] = f\"{prepend_text}\\n{input_file_content.get('text', '')}\"\n",
    "        \n",
    "    # adjust existing sections' start and end positions accordingly\n",
    "    for sec in sections:\n",
    "        sec[\"start\"] += prepend_size + 1  # add 1 to account for newline after abstract \n",
    "        sec[\"end\"] += prepend_size + 1  \n",
    "        \n",
    "    # add new title and abstract sections\n",
    "    sections = [sec_title, sec_abstract] + sections\n",
    "    input_file_content[\"sections\"] = sections\n",
    "           \n",
    "    # also add or replace title and abstract as top-level fields\n",
    "    if (title_text != \"\"):     \n",
    "        input_file_content[\"title\"] = title_text\n",
    "    if(abstract_text != \"\"):\n",
    "        input_file_content[\"abstract\"] = abstract_text\n",
    "    # TODO - write modified sections etc back to original input file?\n",
    "                       \n",
    "    print(f\"processing file {counter}: {entry.name}\")\n",
    "\n",
    "    # set up metadata to include in output\n",
    "    metadata = {\n",
    "        \"identifier\": entry.name,\n",
    "        \"title\": \"vocabulary-based information extraction results\",\n",
    "        \"description\": \"vocabulary-based information extraction annotation on ADS OASIS journal report full-text\",\n",
    "        \"creator\": \"T4_1_2_IE_OASIS_journal_reports.ipynb\",\n",
    "        #\"periodo_authority_id\": periodo_authority_id,\n",
    "        \"pipeline\": nlp.pipe_names,\n",
    "        \"input_file_name\": entry.name,\n",
    "        \"input_record_count\": 1\n",
    "    }\n",
    "\n",
    "    # perform annotation on input text\n",
    "    ts_nlp = DT.now() \n",
    "    print(\"running nlp pipeline\") \n",
    "    doc = nlp(input_file_content.get(\"text\",\"\"))\n",
    "    print(f\"finished nlp pipeline in {DT.now() - ts_nlp}\")\n",
    "\n",
    "    # add calculated scores to spans\n",
    "    sections = list(input_file_content.get(\"sections\", []))\n",
    "    scorer = SpanScorer(nlp, sections=sections)\n",
    "    doc = scorer(doc)\n",
    "\n",
    "    ts_sum = DT.now() \n",
    "    print(\"summarizing results\") \n",
    "    summary = DocSummary(doc, metadata=metadata)\n",
    "    print(f\"finished summarizing results in {DT.now() - ts_sum}\")\n",
    "        \n",
    "    ts_finished = DT.now()\n",
    "    metadata[\"starting\"] = ts_nlp.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    metadata[\"finished\"] = ts_finished.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    metadata[\"duration\"] =  ts_finished - ts_nlp\n",
    "\n",
    "    # write results to text files\n",
    "    # html_file_name = os.path.join(output_directory, f\"ie-output-{slugify(entryname)}.html\") \n",
    "    text_file_name = os.path.join(output_directory, f\"ie-output-{slugify(entry.name)}.txt\")\n",
    "    csv_file_name = os.path.join(output_directory, f\"ie-output-{slugify(entry.name)}.csv\")\n",
    "    json_file_name = os.path.join(output_directory, f\"ie-output-{slugify(entry.name)}.json\")\n",
    "    pdf_file_name = os.path.join(output_directory, f\"ie-output-{slugify(entry.name)}.pdf\")\n",
    "        \n",
    "    ts_csv = DT.now()   \n",
    "    print(\"creating CSV file\")\n",
    "    with open(csv_file_name, \"w\") as file:\n",
    "        file.write(summary.spans_to_csv())\n",
    "    print(f\"finished creating CSV report in {DT.now() - ts_csv}\")\n",
    "\n",
    "    # note early run took 21 mins for 2 files\n",
    "    # 10/07/2025 - 1:10:31 for 10 files (JSON, TXT and HTML output)\n",
    "    # 19/11/2025 - 0:49:45 for 10 files (JSON, CSV and PDF output) (omitting negation pairs)\n",
    "    # 02/02/2026 - 0:41:15 for 18 files (JSON, CSV and PDF output); remaining 2 in 0:15:44\n",
    "    # 04/02/2026 - 0:00:00 for 20 files (JSON, CSV and PDF output)\n",
    "    ts_pdf = DT.now()   \n",
    "    print(\"creating PDF report\")\n",
    "    report = summary.report(format=\"html\")\n",
    "    HTML(None, string=report, encoding=\"utf-8\").write_pdf(target=pdf_file_name)                \n",
    "    print(f\"finished creating PDF report in {DT.now() - ts_pdf}\")\n",
    "\n",
    "    #ts_started = DT.now()\n",
    "    #print(\"creating TEXT report\")\n",
    "    #report = summary.report(format=\"text\")\n",
    "    #with open(text_file_name, \"w\") as file:\n",
    "        #file.write(report)\n",
    "    #print(f\"finished creating TEXT report in {DT.now() - ts_started}\")\n",
    "             \n",
    "    ts_json = DT.now()\n",
    "    print(\"creating JSON report\")\n",
    "    report = summary.report(format=\"json\")\n",
    "    report[\"sections\"] = input_file_content.get(\"sections\", [])  # include sections in output JSON for score diagnostics\n",
    "        \n",
    "    with open(json_file_name, \"w\") as file:\n",
    "        # convert to JSON string first for pretty printing\n",
    "        json_string = json.dumps(report, indent=4, default=str)\n",
    "        file.write(json_string)\n",
    "    print(f\"finished creating JSON report in {DT.now() - ts_json}\")\n",
    "\n",
    "print(f\"finished processing {counter} files in {DT.now() - ts_started}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
