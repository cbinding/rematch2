{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running ATRIUM information extraction pipeline on full text extracted from OASIS PDF reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "# suppress user warnings during execution\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "# load required dependencies\n",
    "%pip install --upgrade pip\n",
    "%pip install spacy\n",
    "%pip install ipywidgets\n",
    "\n",
    "%sx python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning input file directory\n",
      "reading text_extraction_archael547-079-116-ceolwulf.pdf.json\n",
      "read 'text_extraction_archael547-079-116-ceolwulf.pdf.json' in 0:00:00.002980ms\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:02:27.772107\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.006653\n",
      "creating CSV file\n",
      "finished creating CSV file in 0:00:00.024600\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 41.978 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 5.034 seconds\n",
      "\"_get_all_pairs\" ran in 47.013 seconds\n",
      "\"get_span_pairs\" ran in 47.013 seconds\n",
      "\"report_to_html\" ran in 48.709 seconds\n",
      "\"report\" ran in 48.709 seconds\n",
      "finished creating PDF report in 0:00:55.553863\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.203 seconds\n",
      "\"report\" ran in 0.203 seconds\n",
      "finished creating JSON report in 0:00:00.206163\n",
      "reading text_extraction_NAS_20_1985_87-112_Taylor.pdf.json\n",
      "read 'text_extraction_NAS_20_1985_87-112_Taylor.pdf.json' in 0:00:00.005836ms\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:01:11.660283\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.001775\n",
      "creating CSV file\n",
      "finished creating CSV file in 0:00:00.027212\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 24.010 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 2.530 seconds\n",
      "\"_get_all_pairs\" ran in 26.541 seconds\n",
      "\"get_span_pairs\" ran in 26.541 seconds\n",
      "\"report_to_html\" ran in 27.473 seconds\n",
      "\"report\" ran in 27.473 seconds\n",
      "finished creating PDF report in 0:00:34.470020\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.113 seconds\n",
      "\"report\" ran in 0.113 seconds\n",
      "finished creating JSON report in 0:00:00.114877\n",
      "reading text_extraction_archael547-005-040-breeze.pdf.json\n",
      "read 'text_extraction_archael547-005-040-breeze.pdf.json' in 0:00:00.002987ms\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:02:30.710346\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000401\n",
      "creating CSV file\n",
      "finished creating CSV file in 0:00:00.040019\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 77.251 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 9.050 seconds\n",
      "\"_get_all_pairs\" ran in 86.301 seconds\n",
      "\"get_span_pairs\" ran in 86.301 seconds\n",
      "\"report_to_html\" ran in 89.382 seconds\n",
      "\"report\" ran in 89.383 seconds\n",
      "finished creating PDF report in 0:01:38.321550\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.746 seconds\n",
      "\"report\" ran in 0.746 seconds\n",
      "finished creating JSON report in 0:00:00.749926\n",
      "reading text_extraction_DAJ_v023_1901_040-047.pdf.json\n",
      "read 'text_extraction_DAJ_v023_1901_040-047.pdf.json' in 0:00:00.009579ms\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:14.452918\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.004247\n",
      "creating CSV file\n",
      "finished creating CSV file in 0:00:00.005536\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 0.716 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 0.090 seconds\n",
      "\"_get_all_pairs\" ran in 0.806 seconds\n",
      "\"get_span_pairs\" ran in 0.806 seconds\n",
      "\"report_to_html\" ran in 0.841 seconds\n",
      "\"report\" ran in 0.841 seconds\n",
      "finished creating PDF report in 0:00:02.158527\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.024 seconds\n",
      "\"report\" ran in 0.024 seconds\n",
      "finished creating JSON report in 0:00:00.024339\n",
      "reading text_extraction_DAJ_v086_1966_031-053.pdf.json\n",
      "read 'text_extraction_DAJ_v086_1966_031-053.pdf.json' in 0:00:00.000489ms\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:49.932606\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000087\n",
      "creating CSV file\n",
      "finished creating CSV file in 0:00:00.018205\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 10.560 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 1.186 seconds\n",
      "\"_get_all_pairs\" ran in 11.746 seconds\n",
      "\"get_span_pairs\" ran in 11.746 seconds\n",
      "\"report_to_html\" ran in 12.184 seconds\n",
      "\"report\" ran in 12.184 seconds\n",
      "finished creating PDF report in 0:00:16.277645\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.081 seconds\n",
      "\"report\" ran in 0.081 seconds\n",
      "finished creating JSON report in 0:00:00.082381\n",
      "reading text_extraction_surreyac103_063-090_lambert.pdf.json\n",
      "read 'text_extraction_surreyac103_063-090_lambert.pdf.json' in 0:00:00.001868ms\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:01:09.761800\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000251\n",
      "creating CSV file\n",
      "finished creating CSV file in 0:00:00.030140\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 26.439 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 2.841 seconds\n",
      "\"_get_all_pairs\" ran in 29.281 seconds\n",
      "\"get_span_pairs\" ran in 29.281 seconds\n",
      "\"report_to_html\" ran in 30.284 seconds\n",
      "\"report\" ran in 30.284 seconds\n",
      "finished creating PDF report in 0:00:37.970478\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.119 seconds\n",
      "\"report\" ran in 0.119 seconds\n",
      "finished creating JSON report in 0:00:00.120015\n",
      "reading text_extraction_2022_96_013-068_Huxley.pdf.json\n",
      "read 'text_extraction_2022_96_013-068_Huxley.pdf.json' in 0:00:00.135117ms\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:01:45.009552\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000447\n",
      "creating CSV file\n",
      "finished creating CSV file in 0:00:00.060142\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 84.321 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 9.576 seconds\n",
      "\"_get_all_pairs\" ran in 93.898 seconds\n",
      "\"get_span_pairs\" ran in 93.898 seconds\n",
      "\"report_to_html\" ran in 97.091 seconds\n",
      "\"report\" ran in 97.091 seconds\n",
      "finished creating PDF report in 0:01:51.189584\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.232 seconds\n",
      "\"report\" ran in 0.232 seconds\n",
      "finished creating JSON report in 0:00:00.234792\n",
      "reading text_extraction_2022_96_001_012_Cooper_Garton.pdf.json\n",
      "read 'text_extraction_2022_96_001_012_Cooper_Garton.pdf.json' in 0:00:00.001823ms\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:32.101692\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.003240\n",
      "creating CSV file\n",
      "finished creating CSV file in 0:00:00.012409\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 4.607 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 0.433 seconds\n",
      "\"_get_all_pairs\" ran in 5.040 seconds\n",
      "\"get_span_pairs\" ran in 5.040 seconds\n",
      "\"report_to_html\" ran in 5.188 seconds\n",
      "\"report\" ran in 5.188 seconds\n",
      "finished creating PDF report in 0:00:08.916291\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.051 seconds\n",
      "\"report\" ran in 0.051 seconds\n",
      "finished creating JSON report in 0:00:00.051574\n",
      "reading text_extraction_120_031_097.pdf.json\n",
      "read 'text_extraction_120_031_097.pdf.json' in 0:00:00.003229ms\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:03:48.332609\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000213\n",
      "creating CSV file\n",
      "finished creating CSV file in 0:00:00.038849\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 93.892 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 10.111 seconds\n",
      "\"_get_all_pairs\" ran in 104.003 seconds\n",
      "\"get_span_pairs\" ran in 104.004 seconds\n",
      "\"report_to_html\" ran in 107.796 seconds\n",
      "\"report\" ran in 107.796 seconds\n",
      "finished creating PDF report in 0:01:57.098150\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.307 seconds\n",
      "\"report\" ran in 0.307 seconds\n",
      "finished creating JSON report in 0:00:00.310194\n",
      "reading text_extraction_NAS_20_1985_67-86_Jackson.pdf.json\n",
      "read 'text_extraction_NAS_20_1985_67-86_Jackson.pdf.json' in 0:00:00.000757ms\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:54.623396\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.012464\n",
      "creating CSV file\n",
      "finished creating CSV file in 0:00:00.020912\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 13.936 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 1.491 seconds\n",
      "\"_get_all_pairs\" ran in 15.427 seconds\n",
      "\"get_span_pairs\" ran in 15.427 seconds\n",
      "\"report_to_html\" ran in 15.950 seconds\n",
      "\"report\" ran in 15.950 seconds\n",
      "finished creating PDF report in 0:00:21.502099\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.092 seconds\n",
      "\"report\" ran in 0.092 seconds\n",
      "finished creating JSON report in 0:00:00.092909\n"
     ]
    }
   ],
   "source": [
    "import spacy # for text processing\n",
    "from spacy.language import Language\n",
    "from datetime import datetime as DT # for timestamps\n",
    "import json, os\n",
    "from slugify import slugify # for valid filenames from identifiers\n",
    "from weasyprint import HTML\n",
    "#from rematch2 import PeriodoRuler, VocabularyRuler, NegationRuler, DocSummary, TextNormalizer\n",
    "from rematch2 import DocSummary\n",
    "\n",
    "from ATRIUM_T4_1_2_IE_pipeline import get_pipeline\n",
    "\n",
    "# using predefined spaCy pipeline (English)\n",
    "nlp = get_pipeline(\"en\")\n",
    "\n",
    "# input directory containing text files to process\n",
    "#input_directory = \"./data/oasis/journals_july_2024/text extraction - new\" # Mugdha's output\n",
    "input_directory = \"./data/oasis/journals_july_2024/text_extraction-20251117\" # Mark's script re-extracted text 2025-11-17\n",
    "\n",
    "# subset of names of files to process\n",
    "file_names = [\n",
    "    \"text_extraction_120_031_097.pdf.json\",\n",
    "    \"text_extraction_2022_96_001_012_cooper_garton.pdf.json\",\n",
    "    \"text_extraction_2022_96_013-068_huxley.pdf.json\",\n",
    "    \"text_extraction_archael547-005-040-breeze.pdf.json\",\n",
    "    \"text_extraction_archael547-079-116-ceolwulf.pdf.json\",\n",
    "    \"text_extraction_daj_v023_1901_040-047.pdf.json\",\n",
    "    \"text_extraction_daj_v086_1966_031-053.pdf.json\",\n",
    "    \"text_extraction_nas_20_1985_67-86_jackson.pdf.json\",\n",
    "    \"text_extraction_nas_20_1985_87-112_taylor.pdf.json\",\n",
    "    \"text_extraction_surreyac103_063-090_lambert.pdf.json\"\n",
    "]\n",
    "\n",
    "# timestamp for use in directory names\n",
    "timestamp = DT.now().strftime('%Y%m%d')   \n",
    "\n",
    "# create output file path if it does not already exist\n",
    "output_directory = os.path.join(input_directory, f\"ie-output-{timestamp}\")\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "counter = 0\n",
    "print(\"scanning input file directory\")\n",
    "for entry in os.scandir(input_directory):\n",
    "    \n",
    "    #if entry.is_file() and entry.name.endswith(\".txt\"): \n",
    "    if entry.is_file() and entry.name.lower() in file_names:  \n",
    "        # temp break for testing\n",
    "        #counter += 1        \n",
    "        #if counter > 3: \n",
    "            #break\n",
    "\n",
    "        ts_started = DT.now()\n",
    "        \n",
    "        # read text contents of input file\n",
    "        input_file_name = entry.name        \n",
    "        \n",
    "        print(f\"reading {input_file_name}\")\n",
    "        input_file_text = \"\"        \n",
    "        with open(entry.path) as input_file:\n",
    "            if(entry.name.endswith(\".json\")):\n",
    "                data = json.load(input_file)\n",
    "                input_file_text = data.get(\"text\",\"\")\n",
    "            else:\n",
    "                input_file_text = input_file.read()\n",
    "        print(f\"read '{input_file_name}' in {DT.now() - ts_started}ms\")\n",
    "        \n",
    "        # set up metadata to include in output\n",
    "        metadata = {\n",
    "            \"identifier\": input_file_name,\n",
    "            \"title\": \"vocabulary-based information extraction results\",\n",
    "            \"description\": \"vocabulary-based information extraction annotation on ADS OASIS journal report full-text\",\n",
    "            \"creator\": \"T4-1-2-IE-OASIS-reports-full-text.ipynb\",\n",
    "            #\"periodo_authority_id\": periodo_authority_id,\n",
    "            \"pipeline\": nlp.pipe_names,\n",
    "            \"input_file_name\": input_file_name,\n",
    "            \"input_record_count\": 1\n",
    "        }\n",
    "\n",
    "        # perform annotation on input text\n",
    "        ts_nlp = DT.now() \n",
    "        print(\"running nlp pipeline\") \n",
    "        doc = nlp(input_file_text)\n",
    "        print(f\"finished nlp pipeline in {DT.now() - ts_nlp}\")\n",
    "\n",
    "        ts_sum = DT.now() \n",
    "        print(\"summarizing results\") \n",
    "        summary = DocSummary(doc, metadata=metadata)\n",
    "        print(f\"finished summarizing results in {DT.now() - ts_sum}\")\n",
    "        \n",
    "        ts_finished = DT.now()\n",
    "        metadata[\"starting\"] = ts_started.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "        metadata[\"finished\"] = ts_finished.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "        metadata[\"duration\"] =  ts_finished - ts_started\n",
    "\n",
    "        # write results to text files\n",
    "        #html_file_name = os.path.join(output_directory, f\"ner-output-{slugify(input_file_name)}.html\") \n",
    "        #text_file_name = os.path.join(output_directory, f\"ner-output-{slugify(input_file_name)}.txt\")\n",
    "        csv_file_name = os.path.join(output_directory, f\"ner-output-{slugify(input_file_name)}.csv\")\n",
    "        json_file_name = os.path.join(output_directory, f\"ner-output-{slugify(input_file_name)}.json\")\n",
    "        pdf_file_name = os.path.join(output_directory, f\"ner-output-{slugify(input_file_name)}.pdf\")\n",
    "        \n",
    "        ts_started = DT.now()   \n",
    "        print(\"creating CSV file\")\n",
    "        with open(csv_file_name, \"w\") as file:\n",
    "            file.write(summary.spans_to_csv())\n",
    "        print(f\"finished creating CSV file in {DT.now() - ts_started}\")\n",
    "\n",
    "        # note last run took 21 mins for 2 files\n",
    "        # 10/07/25 - now 1:10:31 for 10 files (JSON, TEXT and HTML output)\n",
    "        # 17/11/25 - now 0:25:50 for 10 files (JSON, CSV and PDF output) (now omitting negation pairs)\n",
    "        ts_started = DT.now()   \n",
    "        print(\"creating PDF report\")\n",
    "        report = summary.report(format=\"html\")\n",
    "        HTML(None, string=report, encoding=\"utf-8\").write_pdf(target=pdf_file_name)                \n",
    "        print(f\"finished creating PDF report in {DT.now() - ts_started}\")\n",
    "\n",
    "        #ts_started = DT.now()\n",
    "        #print(\"creating TEXT report\")\n",
    "        #report = summary.report(format=\"text\")\n",
    "        #with open(text_file_name, \"w\") as file:\n",
    "            #file.write(report)\n",
    "        #print(f\"finished creating TEXT report in {DT.now() - ts_started}\")\n",
    "             \n",
    "        ts_started = DT.now()\n",
    "        print(\"creating JSON report\")\n",
    "        report = summary.report(format=\"json\")\n",
    "        with open(json_file_name, \"w\") as file:\n",
    "            file.write(report) \n",
    "        print(f\"finished creating JSON report in {DT.now() - ts_started}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
