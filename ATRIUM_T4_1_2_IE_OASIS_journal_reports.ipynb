{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running ATRIUM information extraction pipeline on full text extracted from OASIS PDF reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "# suppress user warnings during execution\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "# load required dependencies\n",
    "%pip install --upgrade pip\n",
    "%pip install spacy\n",
    "%pip install ipywidgets\n",
    "\n",
    "%sx python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up nlp pipeline\n",
      "finished setting up nlp pipeline in 0:01:45.593784\n",
      "extracting metadata records from CSV file\n",
      "Total metadata records extracted: 75\n",
      "{'title': 'King Ceolwulf’s land grants to St Cuthbert and their loss in the ninth century', 'file': 'archael547-079-116-ceolwulf.pdf', 'object_id': 3226527, 'journal': 'Archaeologia Aeliana', 'volume': 'Volume 47', 'pagees': 'Pages 79:116', 'abstract': 'The Historia de Sancto Cuthberto makes retrospective claims to lands granted to St Cuthbert and, in some cases, later taken from the church. Sections 8 and 11 refer to lands granted by King Ceolwulf (729–737) and lost around 860. Retrogressive technique is used to reconstruct the geography of these lands, within the widely accepted models of shire and minster organisation, drawing on post-Conquest feudal records. The circumstances under which these and other estates were taken from the church in the 9th century are discussed. It is argued that Lindisfarne’s territorial reach in Northumberland declined from a high point under a partnership between Bishop Ecgred (830–845) and King Eanred (c. 820s–850s) in the 860s in the face of the arrival of the Danish army and the collapse of the Northumbrian state. Some of the place-names of Historia 8 and 11 present problems of identification, where generic elements of the names have proved unstable. Early forms of key names are reviewed and discussed.', 'author': 'O’Brien, Colm|Adams, Max|Whaley, Diana', 'publisher': 'Society of Antiquaries of Newcastle', 'date': 2018}\n",
      "scanning input file directory\n",
      "--------------------------------\n",
      "reading text_extraction_surreyac103_091-172_haslam.pdf.json\n",
      "processing file 1: text_extraction_surreyac103_091-172_haslam.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:04:43.686635\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000354\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.113772\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 399.906 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 44.864 seconds\n",
      "\"_get_all_pairs\" ran in 444.771 seconds\n",
      "\"get_span_pairs\" ran in 444.772 seconds\n",
      "\"report_to_html\" ran in 461.534 seconds\n",
      "\"report\" ran in 461.534 seconds\n",
      "finished creating PDF report in 0:08:04.338286\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.533 seconds\n",
      "\"report\" ran in 0.533 seconds\n",
      "finished creating JSON report in 0:00:00.554932\n",
      "--------------------------------\n",
      "reading text_extraction_SAC118_Stevens.pdf.json\n",
      "processing file 2: text_extraction_SAC118_Stevens.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:44.589042\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.003197\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.017399\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 8.040 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 1.047 seconds\n",
      "\"_get_all_pairs\" ran in 9.088 seconds\n",
      "\"get_span_pairs\" ran in 9.088 seconds\n",
      "\"report_to_html\" ran in 9.445 seconds\n",
      "\"report\" ran in 9.445 seconds\n",
      "finished creating PDF report in 0:00:14.204369\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.097 seconds\n",
      "\"report\" ran in 0.097 seconds\n",
      "finished creating JSON report in 0:00:00.102804\n",
      "finished processing 2 files in 0:15:43.730849\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime as DT # for timestamps\n",
    "import json, os\n",
    "from slugify import slugify # for valid filenames from identifiers\n",
    "from weasyprint import HTML\n",
    "#from rematch2 import PeriodoRuler, VocabularyRuler, NegationRuler, DocSummary, TextNormalizer\n",
    "from rematch2 import DocSummary\n",
    "from rematch2.SpanScorer import SpanScorer\n",
    "#from tides_dataclasses import Section\n",
    "\n",
    "from ATRIUM_T4_1_2_IE_pipeline import get_pipeline, get_records_from_csv_file\n",
    "\n",
    "\n",
    "# using predefined spaCy pipeline (English)\n",
    "ts_started = DT.now()     \n",
    "print(\"setting up nlp pipeline\")\n",
    "nlp = get_pipeline(\"en\")\n",
    "print(f\"finished setting up nlp pipeline in {DT.now() - ts_started}\")\n",
    "\n",
    "# input directory containing text files to process\n",
    "#input_directory = \"./data/oasis/journals_july_2024/text extraction - new\" # Mugdha's output\n",
    "input_directory = \"./data/oasis/journals_july_2024/text_extraction_20251117\" # Mark's script re-extracted text 2025-11-17\n",
    "\n",
    "# read separate CSV file containing titles and abstracts for (some of) the files we will process\n",
    "print(\"extracting metadata records from CSV file\")\n",
    "metadata_file_path = os.path.join(input_directory, \"journal_metadata.csv\")\n",
    "metadata_records = get_records_from_csv_file(file_path=metadata_file_path)\n",
    "print(f\"Total metadata records extracted: {len(metadata_records)}\")\n",
    "print(metadata_records[0])  # print first record for inspection\n",
    "\n",
    "# subset of files to process\n",
    "file_names_subset = [\n",
    "\t#\"text_extraction_120_031_097.pdf.json\",\n",
    "\t#\"text_extraction_2022_96_013-068_Huxley.pdf.json\",\n",
    "    #\"text_extraction_078_233_250.pdf.json\",\n",
    "    #\"text_extraction_120_215_235.pdf.json\",\n",
    "    #\"text_extraction_2022_96_001_012_Cooper_Garton.pdf.json\",\n",
    "    #\"text_extraction_2022_96_079-094_Browning_et_al.pdf.json\",\n",
    "    #\"text_extraction_archael522-067-077-whitworth.pdf.json\",\n",
    "    #\"text_extraction_archael547-005-040-breeze.pdf.json\",\n",
    "    #\"text_extraction_archael547-079-116-ceolwulf.pdf.json\",\n",
    "    #\"text_extraction_DAJ_v023_1901_040-047.pdf.json\",\n",
    "    #\"text_extraction_DAJ_v086_1966_093-098.pdf.json\",\n",
    "    #\"text_extraction_DAJ_v106_1986_005-017.pdf.json\",\n",
    "    #\"text_extraction_DAJ_v106_1986_018-100.pdf.json\",\n",
    "    #\"text_extraction_NAS_20_1985_113-138_Shaw.pdf.json\",\n",
    "    #\"text_extraction_SAC118_Bedwin.pdf.json\",\n",
    "    #\"text_extraction_SAC118_Garton.pdf.json\",\n",
    "    \"text_extraction_SAC118_Stevens.pdf.json\",\n",
    "    \"text_extraction_surreyac103_091-172_haslam.pdf.json\",\n",
    "    #\"text_extraction_surreyac103_185-266_saxby.pdf.json\",\n",
    "    #\"text_extraction_surreyac103_297-305_english.pdf.json\",\n",
    "]\n",
    "\n",
    "# timestamp for use in directory names\n",
    "timestamp = ts_started.strftime('%Y%m%d')   \n",
    "\n",
    "# create output file path if it does not already exist\n",
    "output_directory = os.path.join(input_directory, f\"ie-output-{timestamp}\")\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "counter = 0\n",
    "print(\"scanning input file directory\")\n",
    "for entry in os.scandir(input_directory):\n",
    "    \n",
    "    if not entry.is_file(): continue\n",
    "    if not entry.name.lower() in list(map(str.lower, file_names_subset)): continue\n",
    "    # temp break for testing\n",
    "    #if counter >= 2: break\n",
    "    counter += 1        \n",
    "        \n",
    "       \n",
    "    # read contents of input (JSON) file\n",
    "    print(f\"--------------------------------\")\n",
    "    print(f\"reading {entry.name}\")\n",
    "    input_file_content = {}        \n",
    "    with open(entry.path) as input_file:\n",
    "        if(entry.name.endswith(\".json\")):\n",
    "            input_file_content = json.load(input_file)\n",
    "        else:\n",
    "            input_file_content = { \"text\": input_file.read() }\n",
    "         \n",
    "    # find matching metadata record (if it exists) for this file\n",
    "    # the metadata filename is the original PDF file; \n",
    "    # the input file is prefixed \"text_extraction_\" and suffixed \".json\"\n",
    "    # so just find an entry that contains the metadata filename    \n",
    "    metadata_record = next((record for record in metadata_records if record.get(\"file\", \"\").lower() in entry.name.lower()), None)\n",
    "        \n",
    "    # replace sections in input file content with title and abstract from metadata record (if available)\n",
    "    if metadata_record is not None:\n",
    "        title_text = metadata_record.get(\"title\", \"\").strip()\n",
    "        abstract_text = metadata_record.get(\"abstract\", \"\").strip()\n",
    "        \n",
    "        # remove any existing title or abstract sections from input file content            \n",
    "        sections = input_file_content.get(\"sections\", [])\n",
    "        sections = list(filter(lambda sec: sec.get(\"type\", \"\") not in [\"title\", \"abstract\"], sections))\n",
    "\n",
    "        # create new title and abstract sections\n",
    "        sec_title = {\n",
    "            \"type\": \"title\",\n",
    "            \"text\": title_text,\n",
    "            \"start\": 0,\n",
    "            \"end\": len(title_text)\n",
    "        }\n",
    "        sec_abstract = {\n",
    "            \"type\": \"abstract\",\n",
    "            \"text\": abstract_text,\n",
    "            \"start\": sec_title[\"end\"] + 2,\n",
    "            \"end\": sec_title[\"end\"] + 2 + len(abstract_text)\n",
    "        }\n",
    "            \n",
    "        # prepend title and abstract to existing text\n",
    "        prepend_text = f\"{title_text}\\n{abstract_text}\\n\\n\"\n",
    "        prepend_size = len(prepend_text)\n",
    "        input_file_content[\"text\"] = f\"{prepend_text}{input_file_content.get('text', '')}\"\n",
    "        \n",
    "        # adjust existing sections' start and end positions accordingly\n",
    "        for sec in sections:\n",
    "            sec[\"start\"] += prepend_size\n",
    "            sec[\"end\"] += prepend_size\n",
    "        \n",
    "        # add new title and abstract sections\n",
    "        sections = [sec_title, sec_abstract] + sections\n",
    "        input_file_content[\"sections\"] = sections\n",
    "           \n",
    "        # also add or replace title and abstract as top-level fields\n",
    "        if (title_text != \"\"):     \n",
    "            input_file_content[\"title\"] = title_text\n",
    "        if(abstract_text != \"\"):\n",
    "            input_file_content[\"abstract\"] = abstract_text\n",
    "    else:\n",
    "        # no corresponding metadata record - so just use first page text to represent the abstract\n",
    "        pages = list(filter(lambda sec: sec.get(\"type\", \"\") == \"page\", input_file_content.get(\"sections\", [])))\n",
    "        if pages:\n",
    "            first_page = min(pages, key=lambda p: p['start'])\n",
    "            first_page_start = first_page.get(\"start\", 0)\n",
    "            first_page_end = first_page.get(\"end\", 0)\n",
    "            first_page_text = input_file_content.get(\"text\", \"\")[first_page_start:first_page_end]\n",
    "            input_file_content[\"abstract\"] = first_page_text\n",
    "            input_file_content[\"sections\"].append({\n",
    "                \"type\": \"abstract\",\n",
    "                \"text\": first_page_text,\n",
    "                \"start\": first_page_start,\n",
    "                \"end\": first_page_end\n",
    "            })\n",
    "        # TODO - write modified sections etc back to input file?                   \n",
    "    print(f\"processing file {counter}: {entry.name}\")\n",
    "\n",
    "    # set up metadata to include in output\n",
    "    metadata = {\n",
    "        \"identifier\": entry.name,\n",
    "        \"title\": \"vocabulary-based information extraction results\",\n",
    "        \"description\": \"vocabulary-based information extraction annotation on ADS OASIS journal report full-text\",\n",
    "        \"creator\": \"T4_1_2_IE_OASIS_journal_reports.ipynb\",\n",
    "        #\"periodo_authority_id\": periodo_authority_id,\n",
    "        \"pipeline\": nlp.pipe_names,\n",
    "        \"input_file_name\": entry.name,\n",
    "        \"input_record_count\": 1\n",
    "    }\n",
    "\n",
    "    # perform annotation on input text\n",
    "    ts_nlp = DT.now() \n",
    "    print(\"running nlp pipeline\") \n",
    "    doc = nlp(input_file_content.get(\"text\",\"\"))\n",
    "    print(f\"finished nlp pipeline in {DT.now() - ts_nlp}\")\n",
    "\n",
    "    # add calculated scores to spans\n",
    "    sections = list(input_file_content.get(\"sections\", []))\n",
    "    scorer = SpanScorer(nlp, sections=sections)\n",
    "    doc = scorer(doc)\n",
    "\n",
    "    ts_sum = DT.now() \n",
    "    print(\"summarizing results\") \n",
    "    summary = DocSummary(doc, metadata=metadata)\n",
    "    print(f\"finished summarizing results in {DT.now() - ts_sum}\")\n",
    "        \n",
    "    ts_finished = DT.now()\n",
    "    metadata[\"starting\"] = ts_nlp.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    metadata[\"finished\"] = ts_finished.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    metadata[\"duration\"] =  ts_finished - ts_nlp\n",
    "\n",
    "    # write results to text files\n",
    "    # html_file_name = os.path.join(output_directory, f\"ner-output-{slugify(input_file_name)}.html\") \n",
    "    text_file_name = os.path.join(output_directory, f\"ner-output-{slugify(entry.name)}.txt\")\n",
    "    csv_file_name = os.path.join(output_directory, f\"ner-output-{slugify(entry.name)}.csv\")\n",
    "    json_file_name = os.path.join(output_directory, f\"ner-output-{slugify(entry.name)}.json\")\n",
    "    pdf_file_name = os.path.join(output_directory, f\"ner-output-{slugify(entry.name)}.pdf\")\n",
    "        \n",
    "    ts_csv = DT.now()   \n",
    "    print(\"creating CSV file\")\n",
    "    with open(csv_file_name, \"w\") as file:\n",
    "        file.write(summary.spans_to_csv())\n",
    "    print(f\"finished creating CSV report in {DT.now() - ts_csv}\")\n",
    "\n",
    "    # note early run took 21 mins for 2 files\n",
    "    # 10/07/2025 - 1:10:31 for 10 files (JSON, TEXT and HTML output)\n",
    "    # 19/11/2025 - 0:49:45 for 10 files (JSON, CSV and PDF output) (now omitting negation pairs)\n",
    "    # 02/02/2026 - 0:41:15 for 18 files (JSON, CSV and PDF output), remaining 2 in 0:15:44\n",
    "    ts_pdf = DT.now()   \n",
    "    print(\"creating PDF report\")\n",
    "    report = summary.report(format=\"html\")\n",
    "    HTML(None, string=report, encoding=\"utf-8\").write_pdf(target=pdf_file_name)                \n",
    "    print(f\"finished creating PDF report in {DT.now() - ts_pdf}\")\n",
    "\n",
    "    #ts_started = DT.now()\n",
    "    #print(\"creating TEXT report\")\n",
    "    #report = summary.report(format=\"text\")\n",
    "    #with open(text_file_name, \"w\") as file:\n",
    "        #file.write(report)\n",
    "    #print(f\"finished creating TEXT report in {DT.now() - ts_started}\")\n",
    "             \n",
    "    ts_json = DT.now()\n",
    "    print(\"creating JSON report\")\n",
    "    report = summary.report(format=\"json\")\n",
    "        \n",
    "    with open(json_file_name, \"w\") as file:\n",
    "        #file.write(report) `\n",
    "        # convert to JSON string\n",
    "        json_string = json.dumps(report, indent=4)\n",
    "        file.write(json_string)\n",
    "    print(f\"finished creating JSON report in {DT.now() - ts_json}\")\n",
    "\n",
    "print(f\"finished processing {counter} files in {DT.now() - ts_started}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
