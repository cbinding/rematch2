{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running ATRIUM information extraction pipeline on full text extracted from OASIS PDF reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "# suppress user warnings during execution\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "# load required dependencies\n",
    "%pip install --upgrade pip\n",
    "%pip install spacy\n",
    "%pip install ipywidgets\n",
    "\n",
    "%sx python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up nlp pipeline\n",
      "finished setting up nlp pipeline in 0:01:50.979470\n",
      "scanning input file directory\n",
      "reading text_extraction_DAJ_v023_1901_040-047.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:14.143251\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000060\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.014007\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 0.798 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 0.060 seconds\n",
      "\"_get_all_pairs\" ran in 0.859 seconds\n",
      "\"get_span_pairs\" ran in 0.859 seconds\n",
      "\"report_to_html\" ran in 0.930 seconds\n",
      "\"report\" ran in 0.930 seconds\n",
      "finished creating PDF report in 0:00:02.743579\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.022 seconds\n",
      "\"report\" ran in 0.022 seconds\n",
      "finished creating JSON report in 0:00:00.022761\n",
      "reading text_extraction_2022_96_013-068_Huxley.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:01:38.525130\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.000274\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.139132\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 85.201 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 9.595 seconds\n",
      "\"_get_all_pairs\" ran in 94.797 seconds\n",
      "\"get_span_pairs\" ran in 94.797 seconds\n",
      "\"report_to_html\" ran in 98.136 seconds\n",
      "\"report\" ran in 98.136 seconds\n",
      "finished creating PDF report in 0:01:52.319904\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.196 seconds\n",
      "\"report\" ran in 0.196 seconds\n",
      "finished creating JSON report in 0:00:00.198239\n",
      "reading text_extraction_2022_96_001_012_Cooper_Garton.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:29.859286\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.005360\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.011345\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 3.633 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 0.395 seconds\n",
      "\"_get_all_pairs\" ran in 4.028 seconds\n",
      "\"get_span_pairs\" ran in 4.028 seconds\n",
      "\"report_to_html\" ran in 4.200 seconds\n",
      "\"report\" ran in 4.200 seconds\n",
      "finished creating PDF report in 0:00:07.967302\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.047 seconds\n",
      "\"report\" ran in 0.047 seconds\n",
      "finished creating JSON report in 0:00:00.047921\n",
      "reading text_extraction_SAC118_Garton.pdf.json\n",
      "running nlp pipeline\n",
      "finished nlp pipeline in 0:00:21.450596\n",
      "summarizing results\n",
      "finished summarizing results in 0:00:00.001355\n",
      "creating CSV file\n",
      "finished creating CSV report in 0:00:00.008666\n",
      "creating PDF report\n",
      "\"_get_dependency_pairs\" ran in 2.321 seconds\n",
      "\"_get_noun_chunk_pairs\" ran in 0.227 seconds\n",
      "\"_get_all_pairs\" ran in 2.548 seconds\n",
      "\"get_span_pairs\" ran in 2.548 seconds\n",
      "\"report_to_html\" ran in 2.643 seconds\n",
      "\"report\" ran in 2.643 seconds\n",
      "finished creating PDF report in 0:00:05.049640\n",
      "creating JSON report\n",
      "\"report_to_json\" ran in 0.036 seconds\n",
      "\"report\" ran in 0.036 seconds\n",
      "finished creating JSON report in 0:00:00.036339\n",
      "finished processing 0 files in 0:06:44.923034\n"
     ]
    }
   ],
   "source": [
    "import spacy # for text processing\n",
    "from spacy.language import Language\n",
    "from datetime import datetime as DT # for timestamps\n",
    "import json, os\n",
    "from slugify import slugify # for valid filenames from identifiers\n",
    "from weasyprint import HTML\n",
    "#from rematch2 import PeriodoRuler, VocabularyRuler, NegationRuler, DocSummary, TextNormalizer\n",
    "from rematch2 import DocSummary\n",
    "from rematch2.SpanScorer import SpanScorer\n",
    "\n",
    "from ATRIUM_T4_1_2_IE_pipeline import get_pipeline\n",
    "\n",
    "# using predefined spaCy pipeline (English)\n",
    "ts_started = DT.now()     \n",
    "print(\"setting up nlp pipeline\")\n",
    "nlp = get_pipeline(\"en\")\n",
    "print(f\"finished setting up nlp pipeline in {DT.now() - ts_started}\")\n",
    "\n",
    "# input directory containing text files to process\n",
    "#input_directory = \"./data/oasis/journals_july_2024/text extraction - new\" # Mugdha's output\n",
    "input_directory = \"./data/oasis/journals_july_2024/text_extraction-20251117\" # Mark's script re-extracted text 2025-11-17\n",
    "\n",
    "# subset of files to process for 27/11/2025 workshop at ADS\n",
    "file_names = [\n",
    "    #\"text_extraction_120_031_097.pdf.json\",#\n",
    "    \"text_extraction_2022_96_013-068_Huxley.pdf.json\",#\n",
    "    #\"text_extraction_archael547-005-040-breeze.pdf.json\",#\n",
    "    #\"text_extraction_archael547-079-116-ceolwulf.pdf.json\",#\n",
    "    \"text_extraction_DAJ_v023_1901_040-047.pdf.json\",#\n",
    "    #\"text_extraction_DAJ_v106_1986_018-100.pdf.json\",#\n",
    "    \"text_extraction_SAC118_Garton.pdf.json\",\n",
    "    #\"text_extraction_surreyac103_091-172_haslam.pdf.json\",#\n",
    "    #\"text_extraction_surreyac103_185-266_saxby.pdf.json\",#\n",
    "    \"text_extraction_2022_96_001_012_Cooper_Garton.pdf.json\"#\n",
    "]\n",
    "\n",
    "# timestamp for use in directory names\n",
    "timestamp = ts_started.strftime('%Y%m%d')   \n",
    "\n",
    "# create output file path if it does not already exist\n",
    "output_directory = os.path.join(input_directory, f\"ie-output-{timestamp}\")\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "counter = 0\n",
    "print(\"scanning input file directory\")\n",
    "for entry in os.scandir(input_directory):\n",
    "    \n",
    "    if entry.is_file() and entry.name.lower() in list(map(str.lower, file_names)):  \n",
    "        # temp break for testing\n",
    "        #counter += 1        \n",
    "        #if counter > 3: \n",
    "            #break\n",
    "\n",
    "        \n",
    "        # read contents of input (JSON) file\n",
    "        print(f\"reading {entry.name}\")\n",
    "        input_file_content = {}        \n",
    "        with open(entry.path) as input_file:\n",
    "            if(entry.name.endswith(\".json\")):\n",
    "                input_file_content = json.load(input_file)                \n",
    "            else:\n",
    "                input_file_content = { \"text\": input_file.read() }\n",
    "         \n",
    "        # set up metadata to include in output\n",
    "        metadata = {\n",
    "            \"identifier\": entry.name,\n",
    "            \"title\": \"vocabulary-based information extraction results\",\n",
    "            \"description\": \"vocabulary-based information extraction annotation on ADS OASIS journal report full-text\",\n",
    "            \"creator\": \"T4-1-2-IE-OASIS-reports-full-text.ipynb\",\n",
    "            #\"periodo_authority_id\": periodo_authority_id,\n",
    "            \"pipeline\": nlp.pipe_names,\n",
    "            \"input_file_name\": entry.name,\n",
    "            \"input_record_count\": 1\n",
    "        }\n",
    "\n",
    "        # perform annotation on input text\n",
    "        ts_nlp = DT.now() \n",
    "        print(\"running nlp pipeline\") \n",
    "        doc = nlp(input_file_content.get(\"text\",\"\"))\n",
    "        print(f\"finished nlp pipeline in {DT.now() - ts_nlp}\")\n",
    "\n",
    "        # add calculated scores for spans\n",
    "        sections = list(input_file_content.get(\"sections\", []))\n",
    "        scorer = SpanScorer(nlp, sections=sections)\n",
    "        doc = scorer(doc)\n",
    "\n",
    "        ts_sum = DT.now() \n",
    "        print(\"summarizing results\") \n",
    "        summary = DocSummary(doc, metadata=metadata)\n",
    "        print(f\"finished summarizing results in {DT.now() - ts_sum}\")\n",
    "        \n",
    "        ts_finished = DT.now()\n",
    "        metadata[\"starting\"] = ts_nlp.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "        metadata[\"finished\"] = ts_finished.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "        metadata[\"duration\"] =  ts_finished - ts_nlp\n",
    "\n",
    "        # write results to text files\n",
    "        # html_file_name = os.path.join(output_directory, f\"ner-output-{slugify(input_file_name)}.html\") \n",
    "        # text_file_name = os.path.join(output_directory, f\"ner-output-{slugify(entry.name)}.txt\")\n",
    "        csv_file_name = os.path.join(output_directory, f\"ner-output-{slugify(entry.name)}.csv\")\n",
    "        json_file_name = os.path.join(output_directory, f\"ner-output-{slugify(entry.name)}.json\")\n",
    "        pdf_file_name = os.path.join(output_directory, f\"ner-output-{slugify(entry.name)}.pdf\")\n",
    "        \n",
    "        ts_csv = DT.now()   \n",
    "        print(\"creating CSV file\")\n",
    "        with open(csv_file_name, \"w\") as file:\n",
    "            file.write(summary.spans_to_csv())\n",
    "        print(f\"finished creating CSV report in {DT.now() - ts_csv}\")\n",
    "\n",
    "        # note last run took 21 mins for 2 files\n",
    "        # 10/07/25 - now 1:10:31 for 10 files (JSON, TEXT and HTML output)\n",
    "        # 19/11/25 - now 0:49:45 for 10 files (JSON, CSV and PDF output) (now omitting negation pairs)\n",
    "        ts_pdf = DT.now()   \n",
    "        print(\"creating PDF report\")\n",
    "        report = summary.report(format=\"html\")\n",
    "        HTML(None, string=report, encoding=\"utf-8\").write_pdf(target=pdf_file_name)                \n",
    "        print(f\"finished creating PDF report in {DT.now() - ts_pdf}\")\n",
    "\n",
    "        #ts_started = DT.now()\n",
    "        #print(\"creating TEXT report\")\n",
    "        #report = summary.report(format=\"text\")\n",
    "        #with open(text_file_name, \"w\") as file:\n",
    "            #file.write(report)\n",
    "        #print(f\"finished creating TEXT report in {DT.now() - ts_started}\")\n",
    "             \n",
    "        ts_json = DT.now()\n",
    "        print(\"creating JSON report\")\n",
    "        report = summary.report(format=\"json\")\n",
    "        with open(json_file_name, \"w\") as file:\n",
    "            file.write(report) \n",
    "        print(f\"finished creating JSON report in {DT.now() - ts_json}\")\n",
    "\n",
    "print(f\"finished processing {counter} files in {DT.now() - ts_started}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
