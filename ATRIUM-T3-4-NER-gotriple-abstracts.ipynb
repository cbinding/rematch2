{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd08d98",
   "metadata": {},
   "source": [
    "# Applying temporal information extraction techniques to GoTriple abstracts\n",
    "This work has been produced as part of the [ATRIUM](https://atrium-research.eu/) project for Work Package 3 (Facilitating Discoverability of and Access to Humanities Resources), Task 3.4.2 (Translation of textual content). The code performs information extraction on multilingual abstracts sourced from the GoTriple portal, to identify temporal entities - named periods (from a specified authority of the [Perio.do](https://perio.do/) gazetteer) or year spans (phrases such as \"*early 15th century*\") mentioned anywhere within the abstract text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d84fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import warnings\n",
    "# suppress user warnings during execution\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "# load required dependencies\n",
    "%pip install --upgrade pip\n",
    "%pip install spacy\n",
    "%pip install srsly\n",
    "\n",
    "%sx python -m spacy download en_core_web_sm\n",
    "%sx python -m spacy download es_core_news_sm\n",
    "%sx python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e4ffb6",
   "metadata": {},
   "source": [
    "## Obtaining GoTriple abstracts\n",
    "Referencing the workflow [Using GoTriple data for your SSH data science tasks](https://marketplace.sshopencloud.eu/workflow/3rVKH7), there are two main methods to obtain abstracts from the [GoTriple portal](https://www.gotriple.eu/) - either via API calls, or via bulk download of GoTriple metadata records (data dumps). The available [data dumps]( https://zenodo.org/records/15784401) are grouped by domain/discipline - the source code below will download (and cache) an 89MB compressed JSONL GZ format file for the chosen domain 'archeo' (Archaeology and Prehistory). It will read and parse the comporessed JSONL data file, extracting abstracts for the specified language, for subsequent processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef92e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy # for NER processing\n",
    "import srsly # for JSONL serialization/deserialization\n",
    "import requests # for downloading files from URL\n",
    "import urllib.parse # for URL encoding/decoding\n",
    "from rematch2 import DocSummary\n",
    "from rematch2.Util import *\n",
    "from functools import reduce\n",
    "from spacy.language import Language\n",
    "\n",
    "# download file from URL to output path and return filename including path\n",
    "# use a previously cached file if it exists , rather than downloading again\n",
    "def download_file_from_url(url: str, output_path: str=\".\") -> str:    \n",
    "\n",
    "    # ensure the intended output path exists   \n",
    "    if not os.path.exists(output_path): os.makedirs(output_path)\n",
    "\n",
    "    # just use previously cached file if it exists\n",
    "    file_name = url.split(\"?\")[0].split(\"/\")[-1]\n",
    "    file_path = f\"{output_path}/{file_name}\" \n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Using cached file {file_path}\")        \n",
    "    else:\n",
    "        print(f\"Downloading file {url} to {file_path}\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    return file_path\n",
    "\n",
    "# download GoTriple JSONL.GZ file for specified domain to output path and return filename including path\n",
    "def download_gotriple_jsonl_gz_file_for_domain(domain: str=\"\", output_path: str=\".\") -> str:\n",
    "    disciplines = [\n",
    "        \"anthro-bio\", \"anthro-se\", \"archeo\", \"archi\", \"art\", \"class\", \n",
    "        \"demo\", \"droit\", \"eco\", \"edu\", \"envir\", \"genre\", \"geo\", \n",
    "        \"hisphilso\", \"hist\", \"info\", \"lang\", \"litt\", \"manag\", \"museo\", \n",
    "        \"musiq\", \"phil\", \"psy\", \"relig\", \"scipo\", \"socio\", \"stat\"\n",
    "    ]   \n",
    "    discipline = domain.strip().lower()\n",
    "    if discipline not in disciplines:\n",
    "        raise ValueError(f\"Domain '{domain}' not recognised - must be one of {disciplines}\")\n",
    "\n",
    "    url = f\"https://zenodo.org/records/15784401/files/{urllib.parse.quote_plus(discipline)}_merged.jsonl.gz?download=1\"\n",
    "    return download_file_from_url(url, output_path)\n",
    "\n",
    "\n",
    "# retrieve abstracts for specified language from a previously downloaded GoTriple JSONL.GZ file\n",
    "# returns an array of records: [{id, lang, text}, {id, lang, text}] ...]\n",
    "def get_abstracts_from_gotriple_jsonl_gz_file(file_path: str, language: str=\"en\") -> list[dict]:\n",
    "    data: list[dict] = []\n",
    "    for record in srsly.read_gzip_jsonl(file_path, True):\n",
    "        id: str = str(record.get(\"id\", None))\n",
    "        if id is not None:\n",
    "            # get abstract only for specified language            \n",
    "            abstracts: list = record.get(\"abstract\", [])\n",
    "            abstract = next(filter(lambda a: a.get(\"lang\", \"\") == language, abstracts), None)\n",
    "            if abstract is not None:\n",
    "                data.append({\n",
    "                    \"id\": id, \n",
    "                    \"lang\": abstract.get(\"lang\", \"\"), \n",
    "                    \"text\": abstract.get(\"text\", \"\") \n",
    "                })           \n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f74a988",
   "metadata": {},
   "source": [
    "## Information extraction pipeline\n",
    "We are using the open-source [spaCy](https://spacy.io/) Natural Language Processing library but not the default NER pipeline functionality, instead creating language-specific (English, French and Spanish) pipelines consisting of specialised components for performing temporal information extraction, each component then having a language-specific implementation. The pipeline has the following components:\n",
    "\n",
    "* `tok2vec` - tokenizer. Splits the text into separate tokens. Part of the base spaCy pipeline\n",
    "* `tagger` - Part of speech tagger to identify instances of NOUN, VERB etc. Part of the base spaCy pipeline\n",
    "* `normalize_text` - Text normalisation to improve subsequent chances of matching. Normalises spelling, whitestace and punctuation\n",
    "* `parser` - Dependency parser. Part of the base spaCy pipeline\n",
    "* `attribute_ruler` - Part of the base spaCy pipeline\n",
    "* `lemmatizer` - Assigns a base form to tokens to improve matching. Part of the base spaCy pipeline\n",
    "* `ordinal_ruler` - identifies ordinals (e.g. \"*Fifteenth*\", \"*15th*\", \"*15°*\", \"*XV*\")\n",
    "* `dateprefix_ruler` - identifies date prefixes (e.g. \"*Early*\", \"*mediales*\", \"*intorno al*\")\n",
    "* `datesuffix_ruler` - identifies date suffixes (e.g. \"*A.D.*\", \"*après J.C.*\", \"*D.C.*\")\n",
    "* `dateseparator_ruler` - identifies common separators (e.g. \"*to*\", \"*and*\", \"*-*\")\n",
    "* `monthname_ruler` - identifies month names (e.g. \"*March*\", \"*marzo*\", \"*mars*\")\n",
    "* `seasonname_ruler` - identifies season names (e.g. \"*Autumn*\", \"*automne*\", \"*autunno*\", \"*otoño*\")\n",
    "* `yearspan_ruler` - identifies year spans (e.g. \"*Early to mid 15th century*\")\n",
    "* `periodo_ruler` - identifies terms from specifies Perio.do authority (e.g. \"*[Epipaléolithique](http://n2t.net/ark:/99152/p02chr452hz)*\", \"*[Bronze Age](http://n2t.net/ark:/99152/p0kh9ds7q8m)*\", \"*[Prima età imperiale](http://n2t.net/ark:/99152/p03dzfbztr7)*\")\n",
    "* `child_span_remover` - removes match results encompassed by a more specific larger match (e.g. \"*[Bronze Age](http://n2t.net/ark:/99152/p0kh9ds7q8m)*\" within match on \"*[Early Bronze Age](http://n2t.net/ark:/99152/p0kh9ds7q8m)*\")\n",
    "\n",
    "\n",
    "The output will be an array of records each containing a `spans` element which is an array of temporal entities identified in the given text:  \n",
    "\n",
    "```json\n",
    "[{ \"id\": \"123\", \"lang\": \"en\", \"text\": \"xxx\", \"spans\": [] }, ...]\n",
    "```\n",
    "\n",
    "* `id` - identifier of the record from the data source\n",
    "* `lang` - language code indicasting the language of the text\n",
    "* `text` - the text to be processed\n",
    "* `spans` - array of temporal entities identified in the given text. \n",
    "\n",
    "An example individual 'span' element and a description of its properties are shown below:\n",
    "\n",
    "```json\n",
    "{ \"start\": 103, \"end\": 130, \"token_start\": 21, \"token_end\": 25, \"label\": \"YEARSPAN\", \"id\": \"\", \"text\": \"primera mitad del siglo XIX\" }\n",
    "```\n",
    "\n",
    "* `start` - the zero-based starting character position of the identified entity in the text.\n",
    "* `end` - the zero-based position of the character following the identified entity in the text.\n",
    "* `token_start` - the starting token position of the identified entity in the text. A token usually equates to either a word or punctuation. \n",
    "* `token_end` - the ending token position of the identified entity in the text.\n",
    "* `label` - a label indicating the 'type' of entity identified. In this case the value will be either \"YEARSPAN\" (matched on a regular expression pattern) or \"PERIOD\" (matched on a [Perio.do](https://perio.do/en/) named period)\n",
    "* `id` - identifier of the entity identified where applicable (a perio.do match will have an associated identifier)\n",
    "* `text` - the text being searched for matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63d07546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading GoTriple data file for domain 'archeo'\n",
      "Using cached file ./data/gotriple/archeo_merged.jsonl.gz\n",
      "extracting abstracts for language 'en'\n",
      "processing 57449 records (en) from GoTriple abstracts data file\n",
      "processing record 0 of 57449\n",
      "processing record 1000 of 57449\n",
      "processing record 2000 of 57449\n",
      "processing record 3000 of 57449\n",
      "identified 1365 records with spans for language 'en'\n",
      "identified 3584 spans in total for language 'en'\n",
      "writing 1365 abstracts with spans for language 'en' to './data/gotriple/ner-output-gotriple-abstracts-en.jsonl.gz'\n",
      "done\n",
      "extracting abstracts for language 'fr'\n",
      "processing 1066 records (fr) from GoTriple abstracts data file\n",
      "processing record 0 of 1066\n",
      "processing record 1000 of 1066\n",
      "identified 436 records with spans for language 'fr'\n",
      "identified 1482 spans in total for language 'fr'\n",
      "writing 436 abstracts with spans for language 'fr' to './data/gotriple/ner-output-gotriple-abstracts-fr.jsonl.gz'\n",
      "done\n",
      "extracting abstracts for language 'es'\n",
      "processing 2586 records (es) from GoTriple abstracts data file\n",
      "processing record 0 of 2586\n",
      "processing record 1000 of 2586\n",
      "processing record 2000 of 2586\n",
      "identified 1057 records with spans for language 'es'\n",
      "identified 1803 spans in total for language 'es'\n",
      "writing 1057 abstracts with spans for language 'es' to './data/gotriple/ner-output-gotriple-abstracts-es.jsonl.gz'\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# prepare English language spaCy pipeline\n",
    "nlp_en: Language = spacy.load(\"en_core_web_sm\", disable = ['ner'])\n",
    "nlp_en.add_pipe(\"normalize_text\", before = \"parser\")\n",
    "nlp_en.add_pipe(\"yearspan_ruler\", last=True) \n",
    "# using 'HE Cultural Periods' authority\n",
    "nlp_en.add_pipe(\"periodo_ruler\", last=True, config={ \"periodo_authority_id\": \"p0kh9ds\" })\n",
    "nlp_en.add_pipe(\"child_span_remover\", last=True) \n",
    "\n",
    "# prepare French language spaCy pipeline\n",
    "nlp_fr: Language = spacy.load(\"fr_core_news_sm\", disable = ['ner'])\n",
    "nlp_fr.add_pipe(\"normalize_text\", before = \"parser\")\n",
    "nlp_fr.add_pipe(\"yearspan_ruler\", last=True)  \n",
    "# using 'PACTOLS chronology periods used in DOLIA data' authority\n",
    "nlp_fr.add_pipe(\"periodo_ruler\", last=True, config={ \"periodo_authority_id\": \"p02chr4\" })\n",
    "nlp_fr.add_pipe(\"child_span_remover\", last=True) \n",
    "\n",
    "# prepare Spanish language spaCy pipeline\n",
    "nlp_es: Language = spacy.load(\"es_core_news_sm\", disable = ['ner'])\n",
    "nlp_es.add_pipe(\"normalize_text\", before = \"parser\")\n",
    "nlp_es.add_pipe(\"yearspan_ruler\", last=True)  \n",
    "# using 'SIA+ Chrono-Cultural Categories' authority\n",
    "nlp_es.add_pipe(\"periodo_ruler\", last=True, config={ \"periodo_authority_id\": \"p07h9k6\" })\n",
    "nlp_es.add_pipe(\"child_span_remover\", last=True) \n",
    "\n",
    "# prepare I/O paths\n",
    "data_directory: str = \"./data/gotriple\"\n",
    "if not os.path.exists(data_directory): os.makedirs(data_directory)\n",
    "\n",
    "# download GoTriple data file for domain 'archeo' (archeology, history, art history, cultural heritage)\n",
    "print(\"downloading GoTriple data file for domain 'archeo'\")\n",
    "input_file_path = download_gotriple_jsonl_gz_file_for_domain(\"archeo\", data_directory)\n",
    "\n",
    "# process abstracts for each specified language\n",
    "languages = [\"en\", \"fr\", \"es\"]\n",
    "\n",
    "for language in languages:\n",
    "    # extract abstracts from the downloaded file\n",
    "    print(f\"extracting abstracts for language '{language}'\")\n",
    "    abstracts = get_abstracts_from_gotriple_jsonl_gz_file(input_file_path, language)\n",
    "    \n",
    "    #timestamp: str = DT.now().strftime('%Y%m%d')\n",
    "    output_file_name: str = f\"ner-output-gotriple-abstracts-{language}.jsonl.gz\"\n",
    "    output_file_path: str = os.path.join(data_directory, output_file_name)\n",
    "\n",
    "    output_data = []\n",
    "    record_count = len(abstracts)\n",
    "    print(f\"processing {record_count} records ({language}) from GoTriple abstracts data file\")\n",
    "    current_record_index = 0\n",
    "    for record in abstracts: \n",
    "        # progress notification every 1000 records\n",
    "        if current_record_index % 1000 == 0:\n",
    "            print(f\"processing record {current_record_index} of {record_count}\")\n",
    "\n",
    "        # temp break after 3000 records for testing\n",
    "        if current_record_index == 3000:\n",
    "            break\n",
    "        \n",
    "        lang = record.get(\"lang\", \"\")      \n",
    "        text = record.get(\"text\", \"\")        \n",
    "        record[\"spans\"] = []\n",
    "\n",
    "        if(len(text) > 0):\n",
    "            # run the pipeline on the input text\n",
    "            if(lang == \"fr\"):\n",
    "                doc = nlp_fr(text)\n",
    "            elif (lang == \"es\"):\n",
    "                doc = nlp_es(text)\n",
    "            else:\n",
    "                doc = nlp_en(text)             \n",
    "            \n",
    "            # get the spans identified by the pipeline\n",
    "            summary = DocSummary(doc)\n",
    "            record[\"spans\"] = summary.spans_to_list()\n",
    "            \n",
    "        current_record_index += 1 \n",
    "    \n",
    "    counter1 = reduce(lambda acc, r: acc + (1 if len(r.get(\"spans\", [])) > 0 else 0), abstracts, 0)\n",
    "    print(f\"identified {counter1} records with spans for language '{language}'\")\n",
    "    counter2 = reduce(lambda acc, r: acc + len(r.get(\"spans\", [])), abstracts, 0)\n",
    "    print(f\"identified {counter2} spans in total for language '{language}'\")\n",
    "\n",
    "    # write the id, abstract and spans to JSON file (only records with identified spans)\n",
    "    filtered_abstracts = list(filter(lambda rec: len(rec.get(\"spans\", [])) > 0, abstracts))\n",
    "    print (f\"writing {len(filtered_abstracts)} abstracts with spans for language '{language}' to '{output_file_path}'\")         \n",
    "    srsly.write_gzip_jsonl(output_file_path, filtered_abstracts)\n",
    "    print(\"done\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13e1cc",
   "metadata": {},
   "source": [
    "## Results\n",
    "In an experimental run on 09/10/2025 the code extracted a total of 57,449 English, 1,066 French and 2,586 Spanish abstracts. \n",
    "For performance reasons the subsequent processing on the English abstracts was limited to only the first 3,000 records.\n",
    "\n",
    "* 3,584 temporal entities were identified within 1,365 out of 3,000 English abstracts.\n",
    "* 1,482 temporal entities were identified within 436 out of 1,066 French abstracts.\n",
    "* 1,803 temporal entities were identified within 1,057 out of 2,586 Spanish abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7abacb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for file './data/gotriple/ner-output-gotriple-abstracts-en.jsonl.gz':\n",
      "Total records: 1365\n",
      "Total temporal entities identified: 3584\n",
      "Stats for file './data/gotriple/ner-output-gotriple-abstracts-fr.jsonl.gz':\n",
      "Total records: 436\n",
      "Total temporal entities identified: 1482\n",
      "Stats for file './data/gotriple/ner-output-gotriple-abstracts-es.jsonl.gz':\n",
      "Total records: 1057\n",
      "Total temporal entities identified: 1803\n"
     ]
    }
   ],
   "source": [
    "# Get stats on each of the processed files   \n",
    "for language in [\"en\", \"fr\", \"es\"]:\n",
    "    data_directory: str = \"./data/gotriple/\"\n",
    "    data_file_name: str = f\"ner-output-gotriple-abstracts-{language}.jsonl.gz\"\n",
    "    data_file_path: str = os.path.join(data_directory, data_file_name)\n",
    "\n",
    "    # Display the stats for the processed file\n",
    "    if os.path.exists(data_file_path):\n",
    "        print(f\"Stats for file '{data_file_path}':\")\n",
    "        # read the data from the JSONL.GZ file (use read_gzip_jsonl for JSON Lines)\n",
    "        data: list[dict] = []\n",
    "        for record in srsly.read_gzip_jsonl(data_file_path, True):\n",
    "            if record is not None:\n",
    "                data.append(record)\n",
    "\n",
    "        print(f\"Total records: {len(data)}\")\n",
    "        total_spans = sum(len(record.get(\"spans\", [])) for record in data)\n",
    "        print(f\"Total temporal entities identified: {total_spans}\")\n",
    "    else:\n",
    "        print(f\"File '{data_file_path}' does not exist.\")\n",
    "        print(\"No data to display stats for.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a0261a",
   "metadata": {},
   "source": [
    "The output data files were then processed using a separate tool ([yearspans](https://github.com/cbinding/yearspans)) to add `minYear`, `maxYear`, `isoSpan` and `duration` properties to the identified temporal entities - see https://github.com/cbinding/yearspans/blob/main/ATRIUM-T3-2-process-gotriple-ner-abstracts.ipynb for details. Some example results from this further processing are shown below:\n",
    "\n",
    "English:\n",
    "```json\n",
    "{\n",
    "\t\"start\": 786,\n",
    "\t\"end\": 794,\n",
    "\t\"token_start\": 163,\n",
    "\t\"token_end\": 164,\n",
    "\t\"label\": \"PERIOD\",\n",
    "\t\"id\": \"http://n2t.net/ark:/99152/p0kh9dsbd33\",\n",
    "\t\"text\": \"Cold War\",\n",
    "\t\"minYear\": \"1946\",\n",
    "\t\"maxYear\": \"1991\",\n",
    "\t\"isoSpan\": \"1946/1991\",\n",
    "\t\"duration\": 46\n",
    "},\n",
    "{\n",
    "\t\"start\": 81,\n",
    "\t\"end\": 106,\n",
    "\t\"token_start\": 17,\n",
    "\t\"token_end\": 21,\n",
    "\t\"label\": \"YEARSPAN\",\n",
    "\t\"id\": \"\",\n",
    "\t\"text\": \"during the 4th century AD\",\n",
    "\t\"minYear\": \"0301\",\n",
    "\t\"maxYear\": \"0400\",\n",
    "\t\"isoSpan\": \"0301/0400\",\n",
    "\t\"duration\": 100\n",
    "},\n",
    "{\n",
    "    \"start\": 387,\n",
    "\t\"end\": 398,\n",
    "\t\"token_start\": 84,\n",
    "\t\"token_end\": 85,\n",
    "\t\"label\": \"PERIOD\",\n",
    "\t\"id\": \"http://n2t.net/ark:/99152/p0kh9dscbkd\",\n",
    "\t\"text\": \"Middle Ages\",\n",
    "\t\"minYear\": \"1066\",\n",
    "\t\"maxYear\": \"1540\",\n",
    "\t\"isoSpan\": \"1066/1540\",\n",
    "\t\"duration\": 475\n",
    "},\n",
    "{\n",
    "\t\"start\": 1029,\n",
    "\t\"end\": 1051,\n",
    "\t\"token_start\": 204,\n",
    "\t\"token_end\": 208,\n",
    "\t\"label\": \"YEARSPAN\",\n",
    "\t\"id\": \"\",\n",
    "\t\"text\": \"6th and 8th century AD\",\n",
    "\t\"minYear\": \"0501\",\n",
    "\t\"maxYear\": \"0800\",\n",
    "\t\"isoSpan\": \"0501/0800\",\n",
    "\t\"duration\": 300\n",
    "},\n",
    "```\n",
    "\n",
    "Spanish:\n",
    "```json\n",
    "{\n",
    "\t\"start\": 197,\n",
    "\t\"end\": 208,\n",
    "\t\"token_start\": 43,\n",
    "\t\"token_end\": 45,\n",
    "\t\"label\": \"YEARSPAN\",\n",
    "\t\"id\": \"\",\n",
    "\t\"text\": \"1404 y 1403\",\n",
    "\t\"minYear\": \"1403\",\n",
    "\t\"maxYear\": \"1404\",\n",
    "\t\"isoSpan\": \"1403/1404\",\n",
    "\t\"duration\": 2\n",
    "},\n",
    "{\n",
    "\t\"start\": 157,\n",
    "\t\"end\": 167,\n",
    "\t\"token_start\": 27,\n",
    "\t\"token_end\": 28,\n",
    "\t\"label\": \"PERIOD\",\n",
    "\t\"id\": \"http://n2t.net/ark:/99152/p07h9k6f5tz\",\n",
    "\t\"text\": \"Edad Media\",\n",
    "\t\"minYear\": \"0400\",\n",
    "\t\"maxYear\": \"1500\",\n",
    "\t\"isoSpan\": \"0400/1500\",\n",
    "\t\t\"duration\": 1101\n",
    "},\n",
    "{\n",
    "\t\"start\": 517,\n",
    "\t\"end\": 525,\n",
    "\t\"token_start\": 96,\n",
    "\t\"token_end\": 97,\n",
    "\t\"label\": \"YEARSPAN\",\n",
    "\t\"id\": \"\",\n",
    "\t\"text\": \"siglo XI\",\n",
    "\t\"minYear\": \"1001\",\n",
    "\t\"maxYear\": \"1100\",\n",
    "\t\"isoSpan\": \"1001/1100\",\n",
    "\t\"duration\": 100\n",
    "},\n",
    "```\n",
    "\n",
    "French:\n",
    "```json\n",
    "{\n",
    "\t\"start\": 1507,\n",
    "\t\"end\": 1528,\n",
    "\t\"token_start\": 250,\n",
    "\t\"token_end\": 254,\n",
    "\t\"label\": \"YEARSPAN\",\n",
    "\t\"id\": \"\",\n",
    "\t\"text\": \"XIIe siecle av. J.-C.\",\n",
    "\t\"minYear\": \"-1199\",\n",
    "\t\"maxYear\": \"-1100\",\n",
    "\t\"isoSpan\": \"-1199/-1100\",\n",
    "\t\"duration\": 100\n",
    "},\n",
    "{\n",
    "\t\"start\": 74,\n",
    "\t\"end\": 96,\n",
    "\t\"token_start\": 13,\n",
    "\t\"token_end\": 15,\n",
    "\t\"label\": \"PERIOD\",\n",
    "\t\"id\": \"http://n2t.net/ark:/99152/p02chr45b7p\",\n",
    "\t\"text\": \"epoque romaine tardive\",\n",
    "\t\"minYear\": \"0300\",\n",
    "\t\"maxYear\": \"0499\",\n",
    "\t\"isoSpan\": \"0300/0499\",\n",
    "\t\"duration\": 200\n",
    "},\n",
    "```\n",
    "\n",
    "There are better (graphical) ways to visualise these results, such as periods on a timeline, or a histogram of the periods found. As this was only a short exercise to demonstrate use of the information extraction tools the further use and visualisation of the results is left to future work. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
