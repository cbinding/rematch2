{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "090edf3a",
   "metadata": {},
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d84fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/python/3.12.1/lib/python3.12/site-packages (25.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spacy in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (2.2.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.12.1/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: srsly in /usr/local/python/3.12.1/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from srsly) (2.0.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pycld2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.42)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Collecting en-core-web-sm==3.8.0',\n",
       " '  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)',\n",
       " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/12.8 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
       " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m12.8/12.8 MB\\x1b[0m \\x1b[31m81.5 MB/s\\x1b[0m  \\x1b[33m0:00:00\\x1b[0m',\n",
       " '\\x1b[?25h\\x1b[38;5;2m✔ Download and installation successful\\x1b[0m',\n",
       " \"You can now load the package via spacy.load('en_core_web_sm')\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "# suppress user warnings during execution\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "# load required dependencies\n",
    "%pip install --upgrade pip\n",
    "%pip install spacy\n",
    "%pip install srsly\n",
    "#%pip install ipywidgets\n",
    "%pip install -U pycld2\n",
    "%sx python -m spacy download en_core_web_sm\n",
    "#%sx python -m spacy download de_core_news_sm\n",
    "#%sx python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef92e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "#from typing import Iterable\n",
    "import spacy # for NER processing\n",
    "#import time # for sleep\n",
    "import srsly # for JSONL serialization/deserialization\n",
    "from datetime import datetime as DT # for timestamps\n",
    "from urllib.parse import quote_plus # to urlencode a string value\n",
    "from bs4 import BeautifulSoup # to parse data from HTML page responses\n",
    "import requests # for performing url request\n",
    "import pycld2 as cld2  # for text language detection\n",
    "from rematch2 import DocSummary\n",
    "from rematch2.Util import *\n",
    "#from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "# get main language code from text using cld2\n",
    "def detect_language(text: str=\"\") -> str:\n",
    "    is_reliable, text_bytes_found, details = cld2.detect(text)\n",
    "    if is_reliable:\n",
    "        return details[0][1]\n",
    "    return \"en\"\n",
    "\n",
    "\n",
    "def get_abstract_from_gotriple_id(id: str=\"\", lang: str=\"en\") -> str:\n",
    "    # build the URL and get the resource\n",
    "    url = f\"https://www.gotriple.eu/documents/{ quote_plus(id) }\"\n",
    "    return get_abstract_from_gotriple_url(url, lang)\n",
    "\n",
    "# parse language-specific abstract from GoTriple URL request response\n",
    "def get_abstract_from_gotriple_url(url: str=\"\", lang: str=\"en\") -> str:\n",
    "    response = requests.get(url, timeout=30)\n",
    "    # parse out the tag containing abstracts from the response \n",
    "    soup = BeautifulSoup(response.text, features=\"html.parser\")\n",
    "    tag = soup.find(\"script\", id=\"__NEXT_DATA__\")\n",
    "    if tag:\n",
    "        # parse language-specific abstract text from the contents of this tag\n",
    "        meta = json.loads(str(tag.contents[0]))\n",
    "        abstracts = meta.get(\"props\", {}).get(\"pageProps\", {}).get(\"document\", {}).get(\"abstract\", [])        \n",
    "        abstract = next(filter(lambda a: a.get(\"lang\", \"\") == lang, abstracts), {}).get(\"text\", \"\")\n",
    "        return str(abstract).strip()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# download file from URL to output path and return result filename with path\n",
    "# if previously cached file exists then use it rather than downloading again\n",
    "def get_file_from_url(url: str, output_path: str=\".\") -> str:\n",
    "    file_name = url.split(\"?\")[0].split(\"/\")[-1]\n",
    "    file_path = f\"{output_path}/{file_name}\"    \n",
    "    if not os.path.exists(output_path): os.makedirs(output_path)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Using cached file {file_path}\")        \n",
    "    else:\n",
    "        print(f\"Downloading file {url} to {file_path}\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    return file_path\n",
    "\n",
    "\n",
    "# download GoTriple JSONL.GZ file for specified domain to output path and return filename with path\n",
    "def get_gotriple_jsonl_gz_file_for_domain(domain: str, output_path: str=\".\") -> str:\n",
    "    url = f\"https://zenodo.org/records/15784401/files/{domain}_merged.jsonl.gz?download=1\"\n",
    "    return get_file_from_url(url, output_path)\n",
    "\n",
    "\n",
    "# retrieve abstracts from a previously downloaded GoTriple JSONL.GZ file for specified language\n",
    "def get_abstracts_from_gotriple_jsonl_gz_file(file_path: str, language: str=\"en\") -> list[dict]:\n",
    "    data: list[dict] = []\n",
    "    for record in srsly.read_gzip_jsonl(file_path, True):\n",
    "        id: str = str(record.get(\"id\", None))\n",
    "        if id is not None:\n",
    "            # get abstract only for specified language            \n",
    "            abstracts: list = record.get(\"abstract\", [])\n",
    "            abstract = next(filter(lambda a: a.get(\"lang\", \"\") == language, abstracts), None)\n",
    "            if abstract is not None:\n",
    "                data.append({\n",
    "                    \"id\": id, \n",
    "                    \"lang\": abstract.get(\"lang\", \"\"), \n",
    "                    \"text\": abstract.get(\"text\", \"\") \n",
    "                })           \n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d07546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading GoTriple data file for domain 'archeo'\n",
      "Using cached file ./data/ner-input/gotriple/archeo_merged.jsonl.gz\n",
      "extracting abstracts for language 'en'\n",
      "processing 57449 records (en) from GoTriple abstracts data file\n",
      "processing record 0 of 57449\n",
      "processing record 1000 of 57449\n",
      "processing record 2000 of 57449\n",
      "processing record 3000 of 57449\n",
      "processing record 4000 of 57449\n",
      "processing record 5000 of 57449\n",
      "writing abstracts for language 'en' to local file\n",
      "extracting abstracts for language 'fr'\n",
      "processing 1066 records (fr) from GoTriple abstracts data file\n",
      "processing record 0 of 1066\n",
      "processing record 1000 of 1066\n",
      "writing abstracts for language 'fr' to local file\n",
      "extracting abstracts for language 'es'\n",
      "processing 2586 records (es) from GoTriple abstracts data file\n",
      "processing record 0 of 2586\n",
      "processing record 1000 of 2586\n",
      "processing record 2000 of 2586\n",
      "writing abstracts for language 'es' to local file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# prepare English language spaCy pipeline\n",
    "nlp_en: Language = spacy.load(\"en_core_web_sm\", disable = ['ner'])\n",
    "nlp_en.add_pipe(\"normalize_text\", before = \"parser\")\n",
    "nlp_en.add_pipe(\"yearspan_ruler\", last=True)  \n",
    "# using 'HE Cultural Periods' authority\n",
    "nlp_en.add_pipe(\"periodo_ruler\", last=True, config={ \"periodo_authority_id\": \"p0kh9ds\" })\n",
    "nlp_en.add_pipe(\"child_span_remover\", last=True) \n",
    "\n",
    "# prepare French language spaCy pipeline\n",
    "nlp_fr: Language = spacy.load(\"fr_core_news_sm\", disable = ['ner'])\n",
    "nlp_fr.add_pipe(\"normalize_text\", before = \"parser\")\n",
    "nlp_fr.add_pipe(\"yearspan_ruler\", last=True)  \n",
    "# using 'PACTOLS chronology periods used in DOLIA data' authority\n",
    "nlp_fr.add_pipe(\"periodo_ruler\", last=True, config={ \"periodo_authority_id\": \"p02chr4\" })\n",
    "nlp_fr.add_pipe(\"child_span_remover\", last=True) \n",
    "\n",
    "# prepare Spanish language spaCy pipeline\n",
    "nlp_es: Language = spacy.load(\"es_core_news_sm\", disable = ['ner'])\n",
    "nlp_es.add_pipe(\"normalize_text\", before = \"parser\")\n",
    "nlp_es.add_pipe(\"yearspan_ruler\", last=True)  \n",
    "# using 'SIA+ Chrono-Cultural Categories' authority\n",
    "nlp_es.add_pipe(\"periodo_ruler\", last=True, config={ \"periodo_authority_id\": \"p07h9k6\" })\n",
    "nlp_es.add_pipe(\"child_span_remover\", last=True) \n",
    "\n",
    "# prepare I/O paths\n",
    "input_directory: str = \"./data/ner-input/gotriple\"\n",
    "if not os.path.exists(input_directory): os.makedirs(input_directory)\n",
    "output_directory: str = \"./data/ner-output/gotriple\"\n",
    "if not os.path.exists(output_directory): os.makedirs(output_directory)\n",
    "\n",
    "# download GoTriple data file for domain 'archeo' (archeology, history, art history, cultural heritage)\n",
    "print(\"downloading GoTriple data file for domain 'archeo'\")\n",
    "input_file_path = get_gotriple_jsonl_gz_file_for_domain(\"archeo\", input_directory)\n",
    "\n",
    "# process abstracts for each language\n",
    "languages = [\"en\", \"fr\", \"es\"]\n",
    "\n",
    "for language in languages:\n",
    "    # extract abstracts from the downloaded file\n",
    "    print(f\"extracting abstracts for language '{language}'\")\n",
    "    abstracts = get_abstracts_from_gotriple_jsonl_gz_file(input_file_path, language)\n",
    "\n",
    "    #print (f\"writing abstracts for language '{language}' to local file\")  \n",
    "    #with open(f\"./data/gotriple/abstracts-{language}.json\", \"w\") as file:\n",
    "        #json.dump(abstracts, file, indent=4)\n",
    "\n",
    "    timestamp: str = DT.now().strftime('%Y%m%d')\n",
    "    output_file_name: str = f\"ner-output-gotriple-abstracts-{language}-{timestamp}.json\"\n",
    "    output_file_path: str = os.path.join(output_directory, output_file_name)\n",
    "\n",
    "    output_data = []\n",
    "    record_count = len(abstracts)\n",
    "    print(f\"processing {record_count} records ({language}) from GoTriple abstracts data file\")\n",
    "    current_record_index = 0\n",
    "    for record in abstracts: \n",
    "        # progress notification every 1000 records\n",
    "        if current_record_index % 1000 == 0:\n",
    "            print(f\"processing record {current_record_index} of {record_count}\")\n",
    "\n",
    "        # temp break after 5000 records for testing\n",
    "        if current_record_index == 5000:\n",
    "            break\n",
    "        \n",
    "        lang = record.get(\"lang\", \"\")      \n",
    "        text = record.get(\"text\", \"\")        \n",
    "        record[\"spans\"] = []\n",
    "\n",
    "        if(len(text) > 0):\n",
    "            # run the pipeline on the input text\n",
    "            if(lang == \"fr\"):\n",
    "                doc = nlp_fr(text)\n",
    "            elif (lang == \"es\"):\n",
    "                doc = nlp_es(text)\n",
    "            else:\n",
    "                doc = nlp_en(text)             \n",
    "            \n",
    "            summary = DocSummary(doc)\n",
    "            record[\"spans\"] = summary.spans_to_list()\n",
    "            \n",
    "        current_record_index += 1 \n",
    "    \n",
    "    print (f\"writing abstracts for language '{language}' to local file\")  \n",
    "    with open(output_file_path, \"w\") as file:\n",
    "        json.dump(abstracts, file, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
