{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running ATRIUM NER pipeline on full text extracted from OASIS PDF reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import warnings\n",
    "# suppress user warnings during execution\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "# load required dependencies\n",
    "%pip install --upgrade pip\n",
    "%pip install spacy\n",
    "%pip install ipywidgets\n",
    "%sx python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy # for NER processing\n",
    "from spacy.tokens import Doc # for NER results\n",
    "from datetime import datetime as DT # for timestamps\n",
    "import os\n",
    "from slugify import slugify # for valid filenames from identifiers\n",
    "from rematch2 import PeriodoRuler, VocabularyRuler, NegationRuler, DocSummary, StringCleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing '078_047_054_new.txt'\n",
      "processing '078_174_204_new.txt'\n",
      "processing '078_216_226_new.txt'\n",
      "processing '078_233_250_new.txt'\n",
      "processing '078_264_270_new.txt'\n",
      "processing '078_391_396_new.txt'\n",
      "processing '120_001_030_new.txt'\n"
     ]
    }
   ],
   "source": [
    "# using predefined spaCy pipeline (English)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable = ['ner'])\n",
    "\n",
    "# using HE Periods list\n",
    "periodo_authority_id = \"p0kh9ds\" \n",
    "\n",
    "# add rematch2 NER component(s) to the end of the pipeline\n",
    "nlp.add_pipe(\"yearspan_ruler\", last=True)    \n",
    "nlp.add_pipe(\"periodo_ruler\", last=True, config={\"periodo_authority_id\": periodo_authority_id}) \n",
    "nlp.add_pipe(\"fish_archobjects_ruler\", last=True)\n",
    "nlp.add_pipe(\"fish_monument_types_ruler\", last=True)  \n",
    "nlp.add_pipe(\"fish_supplementary_ruler\", last=True) \n",
    "nlp.add_pipe(\"negation_ruler\", last=True) \n",
    "nlp.add_pipe(\"child_span_remover\", last=True) \n",
    "\n",
    "input_directory = \"./data/journals_july_2024/text extraction - new\"\n",
    "\n",
    "# create output file path if it does not already exist\n",
    "yyyymmdd = DT.now().strftime('%Y%m%d')        \n",
    "output_directory = os.path.join(input_directory, f\"ner-output-{yyyymmdd}\")\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "counter = 0\n",
    "for entry in os.scandir(input_directory):        \n",
    "    counter += 1\n",
    "    # temp break for testing\n",
    "    if counter > 7: \n",
    "        break\n",
    "    \n",
    "    if entry.is_file() and entry.name.endswith(\".txt\"): \n",
    "        # print progress indicator\n",
    "        input_file_name = entry.name        \n",
    "        print(f\"processing '{input_file_name}'\")\n",
    "\n",
    "        # read text contents of input file\n",
    "        input_file_text = \"\"\n",
    "        with open(entry.path) as input_file:\n",
    "            input_file_text = input_file.read()\n",
    "\n",
    "        # set up metadata to include in output\n",
    "        metadata = {\n",
    "            \"identifier\": input_file_name,\n",
    "            \"title\": \"vocabulary-based NER results\",\n",
    "            \"description\": \"vocabulary-based NER annotation on report full-text\",\n",
    "            \"creator\": \"T4-1-2-NER-OASIS-reports-full-text.ipynb\",\n",
    "            \"created\": DT.now().strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            \"periodo_authority_id\": periodo_authority_id,\n",
    "            \"ner_pipeline\": nlp.pipe_names,\n",
    "            \"input_file_name\": input_file_name,\n",
    "            \"input_record_count\": 1\n",
    "        }\n",
    "\n",
    "        # normalise input text prior to annotation\n",
    "        clean_file_text = StringCleaning.normalize(input_file_text)\n",
    "\n",
    "        # perform annotation on cleaned text    \n",
    "        doc = nlp(clean_file_text)\n",
    "        summary = DocSummary(doc, metadata=metadata)\n",
    "\n",
    "        # write results to text files\n",
    "        html_file_name = os.path.join(output_directory, f\"{slugify(input_file_name)}-ner-output.html\") \n",
    "        text_file_name = os.path.join(output_directory, f\"{slugify(input_file_name)}-ner-output.txt\")\n",
    "        json_file_name = os.path.join(output_directory, f\"{slugify(input_file_name)}-ner-output.json\")\n",
    "        \n",
    "        # note last run took 21 mins for 2 files  \n",
    "        report = summary.report(format=\"html\")      \n",
    "        with open(html_file_name, \"w\") as file:\n",
    "            file.write(report)\n",
    "\n",
    "        report = summary.report(format=\"text\")\n",
    "        with open(text_file_name, \"w\") as file:\n",
    "            file.write(report)\n",
    "             \n",
    "        report = summary.report(format=\"json\")\n",
    "        with open(json_file_name, \"w\") as file:\n",
    "            file.write(summary.report(format=\"json\")) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
