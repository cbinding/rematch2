{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running ATRIUM NER pipeline on full text extracted from OASIS PDF reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import warnings\n",
    "# suppress user warnings during execution\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "# load required dependencies\n",
    "%pip install --upgrade pip\n",
    "%pip install spacy\n",
    "%pip install ipywidgets\n",
    "%sx python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # for NER processing\n",
    "#from spacy.tokens import Doc # for NER results\n",
    "from datetime import datetime as DT # for timestamps\n",
    "import json, os\n",
    "from slugify import slugify # for valid filenames from identifiers\n",
    "from weasyprint import HTML\n",
    "#from rematch2 import PeriodoRuler, VocabularyRuler, NegationRuler, DocSummary, TextNormalizer\n",
    "from rematch2 import DocSummary\n",
    "from rematch2.spacypatterns import patterns_en_ATTRIBUTE_RULES # rules to override POS tags in some cases\n",
    "\n",
    "# using predefined spaCy pipeline (English)\n",
    "print(\"loading spacy language model\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable = ['ner'])\n",
    "print(\"..done\")\n",
    "# adding custom rules to override default POS tagging for specific cases\n",
    "# NOTE: adding rules to existing attribute_ruler component didn't seem to\n",
    "# work, so inserting another one directly after it and adding rules to that\n",
    "#nlp.get_pipe(\"attribute_ruler\").add_patterns(patterns_en_ATTRIBUTE_RULES)\n",
    "print(\"adding attribute ruler\")\n",
    "ar = nlp.add_pipe(\"attribute_ruler\", name=\"custom_attribute_ruler\", after=\"attribute_ruler\")\n",
    "ar.add_patterns(patterns_en_ATTRIBUTE_RULES)\n",
    "print(\"..done\")\n",
    "\n",
    "# using HE Periods list\n",
    "periodo_authority_id = \"p0kh9ds\" \n",
    "\n",
    "# reading supplementary lists from JSON files\n",
    "def read_json(file_name):\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_name, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Problem reading \\\"{file_name}\\\": {e}\")\n",
    "    return data\n",
    "\n",
    "print(\"reading supplementary lists\")\n",
    "supp_list_obj = read_json(\"./supp_list_en_FISH_ARCHOBJECTS.json\")\n",
    "supp_list_mon = read_json(\"./supp_list_en_FISH_MONUMENTS.json\")\n",
    "supp_list_per = read_json(\"./supp_list_en_FISH_PERIODS.json\")\n",
    "print(\"..done\")\n",
    "\n",
    "# lists of vocabulary concepts we don't want to appear in the results even if legitimately matches \n",
    "# stop list is hardcoded directly here, but should also be read in from JSON files as above\n",
    "stop_list_mon = [\n",
    "    \"http://purl.org/heritagedata/schemes/eh_tmt2/concepts/71054\",  # Roundhouse (Lock Up)\n",
    "    \"http://purl.org/heritagedata/schemes/eh_tmt2/concepts/93179\",  # Roundhouse (Railway)\n",
    "    \"http://purl.org/heritagedata/schemes/eh_tmt2/concepts/69434\",  # works\n",
    "    \"http://purl.org/heritagedata/schemes/eh_tmt2/concepts/92230\",  # term\n",
    "    \"http://purl.org/heritagedata/schemes/eh_tmt2/concepts/93931\",  # model\n",
    "    \"http://purl.org/heritagedata/schemes/eh_tmt2/concepts/88215\",  # half moon\n",
    "    \"http://purl.org/heritagedata/schemes/eh_tmt2/concepts/68762\",  # crest\n",
    "]\n",
    "\n",
    "stop_list_obj = [\n",
    "    \"http://purl.org/heritagedata/schemes/mda_obj/concepts/139085\", # Coin (Contemporary Imitation)\n",
    "    \"http://purl.org/heritagedata/schemes/mda_obj/concepts/139087\", # Coin (Modern Forgery)\n",
    "    \"http://purl.org/heritagedata/schemes/mda_obj/concepts/139086\", # Coin (Modern Imitation)\n",
    "    \"http://purl.org/heritagedata/schemes/mda_obj/concepts/95353\",  # level\n",
    "    \"http://purl.org/heritagedata/schemes/mda_obj/concepts/95306\",  # scale\n",
    "    \"http://purl.org/heritagedata/schemes/mda_obj/concepts/96615\",  # shift\n",
    "    \"http://purl.org/heritagedata/schemes/mda_obj/concepts/96379\",  # point\n",
    "    \"http://purl.org/heritagedata/schemes/mda_obj/concepts/143243\", # setting\n",
    "    \"http://purl.org/heritagedata/schemes/mda_obj/concepts/96735\",  # staff\n",
    "    \"http://purl.org/heritagedata/schemes/mda_obj/concepts/100151\", # paper\n",
    "    \"http://purl.org/heritagedata/schemes/mda_obj/concepts/96473\",  # model\n",
    "    \"http://purl.org/heritagedata/schemes/mda_obj/concepts/100107\", # desk\n",
    "    \"http://purl.org/heritagedata/schemes/mda_obj/concepts/95183\",  # crest\n",
    "]\n",
    "\n",
    "# add rematch2 NER component(s) to the pipeline\n",
    "nlp.add_pipe(\"normalize_whitespace\", before = \"tagger\")\n",
    "nlp.add_pipe(\"normalize_punctuation\", before = \"tagger\")\n",
    "nlp.add_pipe(\"normalize_spelling\", before = \"tagger\")\n",
    "\n",
    "nlp.add_pipe(\"yearspan_ruler\", last=True)   \n",
    "nlp.add_pipe(\"periodo_ruler\", last=True, config={\"periodo_authority_id\": periodo_authority_id, \"supp_list\": supp_list_per}) \n",
    "nlp.add_pipe(\"fish_archobjects_ruler\", last=True, config={\"supp_list\": supp_list_obj, \"stop_list\": stop_list_obj}) \n",
    "nlp.add_pipe(\"fish_monument_types_ruler\", last=True, config={\"supp_list\": supp_list_mon, \"stop_list\": stop_list_mon})   \n",
    "nlp.add_pipe(\"negation_ruler\", last=True) \n",
    "nlp.add_pipe(\"child_span_remover\", last=True) \n",
    "\n",
    "input_directory = \"./data/journals_july_2024/text extraction - new\"\n",
    "\n",
    "# subset of files to process\n",
    "file_names = [\n",
    "    \"archael547-079-116-ceolwulf_new.txt\",\n",
    "    \"archael547-005-040-breeze_new.txt\",\n",
    "    \"2022_96_013-068_huxley_new.txt\",\n",
    "    \"2022_96_001_012_cooper_garton_new.txt\",\n",
    "    #\"surreyac103_063-090_lambert_new.txt\",\n",
    "    \"nas_20_1985_67-86_jackson_new.txt\",\n",
    "    \"nas_20_1985_87-112_taylor_new.txt\",\n",
    "    \"daj_v023_1901_040-047_new.txt\",\n",
    "    \"daj_v086_1966_031-053_new.txt\",\n",
    "    \"120_031_097_new.txt\"\n",
    "]\n",
    "\n",
    "# timestamp for use in directory names\n",
    "timestamp = DT.now().strftime('%Y%m%d')   \n",
    "\n",
    "# create output file path if it does not already exist\n",
    "output_directory = os.path.join(input_directory, f\"ner-output-{timestamp}\")\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "counter = 0\n",
    "print(\"scanning input file directory\")\n",
    "for entry in os.scandir(input_directory):        \n",
    "    counter += 1\n",
    "    # temp break for testing\n",
    "    #if counter > 2: \n",
    "        #break\n",
    "    \n",
    "    #if entry.is_file() and entry.name.endswith(\".txt\"): \n",
    "    if entry.is_file() and entry.name.lower() in file_names:    \n",
    "        ts_started = DT.now()   \n",
    "\n",
    "        # print progress indicator\n",
    "        input_file_name = entry.name        \n",
    "        print(f\"processing entry {counter} - '{input_file_name}'\")\n",
    "\n",
    "        # read text contents of input file\n",
    "        print(f\"reading {input_file_name}\")\n",
    "        input_file_text = \"\"\n",
    "        with open(entry.path) as input_file:\n",
    "            input_file_text = input_file.read()\n",
    "        print(f\"read '{input_file_name}' in {DT.now() - ts_started}ms\")\n",
    "        \n",
    "        # set up metadata to include in output\n",
    "        metadata = {\n",
    "            \"identifier\": input_file_name,\n",
    "            \"title\": \"vocabulary-based NER results\",\n",
    "            \"description\": \"vocabulary-based NER annotation on report full-text\",\n",
    "            \"creator\": \"T4-1-2-NER-OASIS-reports-full-text.ipynb\",\n",
    "            \"periodo_authority_id\": periodo_authority_id,\n",
    "            \"ner_pipeline\": nlp.pipe_names,\n",
    "            \"input_file_name\": input_file_name,\n",
    "            \"input_record_count\": 1\n",
    "        }\n",
    "\n",
    "        # perform annotation on input text\n",
    "        ts_nlp = DT.now() \n",
    "        print(\"running nlp pipeline\") \n",
    "        doc = nlp(input_file_text)\n",
    "        print(f\"finished nlp pipeline in {DT.now() - ts_nlp}\")\n",
    "\n",
    "        ts_sum = DT.now() \n",
    "        print(\"summarizing results\") \n",
    "        summary = DocSummary(doc, metadata=metadata)\n",
    "        print(f\"finished summarizing results in {DT.now() - ts_sum}\")\n",
    "        \n",
    "        ts_finished = DT.now()\n",
    "\n",
    "        metadata[\"starting\"] = ts_started.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "        metadata[\"finished\"] = ts_finished.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "        metadata[\"duration\"] =  ts_finished - ts_started\n",
    "\n",
    "        # write results to text files\n",
    "        #html_file_name = os.path.join(output_directory, f\"ner-output-{slugify(input_file_name)}.html\") \n",
    "        text_file_name = os.path.join(output_directory, f\"ner-output-{slugify(input_file_name)}.txt\")\n",
    "        json_file_name = os.path.join(output_directory, f\"ner-output-{slugify(input_file_name)}.json\")\n",
    "        pdf_file_name= os.path.join(output_directory, f\"ner-output-{slugify(input_file_name)}.pdf\")\n",
    "        \n",
    "        # note last run took 21 mins for 2 files\n",
    "        # 10/07/25 - now 1:10:31 for 10 files (JSON, TEXT and HTML output)\n",
    "        ts_started = DT.now()   \n",
    "        print(\"creating PDF report\")\n",
    "        report = summary.report(format=\"html\")      \n",
    "        #with open(html_file_name, \"w\") as file:\n",
    "            #file.write(report)\n",
    "        report = summary.report(format=\"html\")      \n",
    "        HTML(None, string=report, encoding=\"utf-8\").write_pdf(target=pdf_file_name)\n",
    "                \n",
    "        print(f\"finished creating PDF report in {DT.now() - ts_started}\")\n",
    "\n",
    "        #ts_started = DT.now()\n",
    "        #print(\"creating TEXT report\")\n",
    "        #report = summary.report(format=\"text\")\n",
    "        #with open(text_file_name, \"w\") as file:\n",
    "            #file.write(report)\n",
    "        #print(f\"finished creating TEXT report in {DT.now() - ts_started}\")\n",
    "             \n",
    "        ts_started = DT.now()\n",
    "        print(\"creating JSON report\")\n",
    "        report = summary.report(format=\"json\")\n",
    "        with open(json_file_name, \"w\") as file:\n",
    "            file.write(report) \n",
    "        print(f\"finished creating JSON report in {DT.now() - ts_started}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
